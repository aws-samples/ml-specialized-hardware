{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# AWS Neuron Inference Demo: Qwen-2.5-7B-Instruct Model\n",
    "\n",
    "## What is AWS Neuron?\n",
    "AWS Neuron is a specialized SDK and runtime for running machine learning inference on **AWS Inferentia** and **Trainium** chips - purpose-built silicon optimized for ML workloads. Unlike general-purpose GPUs, these chips are designed specifically for inference, offering:\n",
    "\n",
    "- **Better cost-performance**: Up to 70% lower cost per inference vs comparable GPU instances\n",
    "- **Predictable performance**: Consistent latency without the variability of shared GPU resources  \n",
    "- **High throughput**: Optimized for batch inference workloads\n",
    "\n",
    "## What is NeuronX Distributed Inference (NxDI)?\n",
    "NxDI is PyTorch-based library that simplifies deploying large language models on Neuron hardware. It provides:\n",
    "- **Production-ready models** (Llama, Qwen, Mixtral, etc.)\n",
    "- **Advanced inference features** (continuous batching, speculative decoding, KV caching)\n",
    "- **Distributed strategies** (tensor parallelism across multiple Neuron cores)\n",
    "- **Seamless integration** with existing PyTorch workflows\n",
    "\n",
    "## Key Concepts You'll Learn:\n",
    "- **Model Compilation**: Converting PyTorch models to Neuron-optimized format (one-time process)\n",
    "- **Tensor Parallelism**: Splitting model layers across multiple Neuron cores for larger models\n",
    "- **Bucketing**: Pre-compiling for different sequence lengths to avoid recompilation\n",
    "- **On-device Sampling**: Performing text generation sampling directly on Neuron hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports\n",
    "\n",
    "First, we'll set up the environment and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download, login\n",
    "\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "\n",
    "def validate_environment():\n",
    "    \"\"\"Validate that we're running on a Neuron-enabled instance.\"\"\"\n",
    "    try:\n",
    "        import torch_neuronx\n",
    "        import neuronx_distributed_inference\n",
    "        print(\"‚úÖ Neuron environment validated\")\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Neuron environment not found: {e}\")\n",
    "        print(\"üí° Make sure you're running on an inf2/trn1 instance with Neuron SDK installed\")\n",
    "        return False\n",
    "\n",
    "if not validate_environment():\n",
    "    raise RuntimeError(\"Please run this notebook on a Neuron-enabled instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## 2. Instance Configuration and Neuron Concepts\n",
    "\n",
    "### Understanding Neuron Cores\n",
    "Each AWS Inferentia/Trainium instance contains multiple **Neuron cores** - the compute units that execute your model:\n",
    "- **inf2.xlarge**: 2 cores (good for development/testing)\n",
    "- **inf2.8xlarge**: 2 cores (cost-effective production)  \n",
    "- **inf2.24xlarge**: 12 cores (high-throughput production)\n",
    "- **inf2.48xlarge**: 24 cores (maximum single-instance performance)\n",
    "\n",
    "### Tensor Parallelism (TP)\n",
    "For models too large for a single core, we split them across multiple cores using **tensor parallelism**:\n",
    "- TP degree = number of cores to use\n",
    "- Higher TP = can run larger models, but with communication overhead\n",
    "- For Qwen2.5-7B: TP=2 is optimal balance of performance and resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ  INSTANCE SELECTION  üîπ\n",
    "# ---------------------------------------------------------------------\n",
    "# Supported instances and their Neuron-core counts\n",
    "INSTANCE_PROFILES = {\n",
    "    \"inf2.xlarge\"   : dict(cores=2 , tp=2 , batch_size=1),\n",
    "    \"inf2.8xlarge\"  : dict(cores=2, tp=2 , batch_size=1),\n",
    "    \"inf2.24xlarge\" : dict(cores=12, tp=12 , batch_size=4),\n",
    "    \"inf2.48xlarge\" : dict(cores=24, tp=24 , batch_size=8),\n",
    "    \"trn1.32xlarge\" : dict(cores=32, tp=32 , batch_size=16),\n",
    "}\n",
    "\n",
    "# Choose your target instance here (or via environment variable)\n",
    "INSTANCE_TYPE   = \"inf2.8xlarge\"\n",
    "assert INSTANCE_TYPE in INSTANCE_PROFILES, f\"Unsupported instance {INSTANCE_TYPE}\"\n",
    "\n",
    "profile         = INSTANCE_PROFILES[INSTANCE_TYPE]\n",
    "NUM_CORES       = profile[\"cores\"]\n",
    "TP_DEGREE       = profile[\"tp\"]\n",
    "BATCH_SIZE      = profile[\"batch_size\"]\n",
    "\n",
    "print(f\"üñ•Ô∏è  Target instance : {INSTANCE_TYPE} \"\n",
    "      f\"(Neuron cores={NUM_CORES}, tp={TP_DEGREE}, batch_size={BATCH_SIZE})\")\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# Paths ----------------------------------------------------------------\n",
    "MODEL_ID              = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "BASE_DIR              = Path(\"/home/ubuntu\")\n",
    "ORIGINAL_MODEL_PATH   = BASE_DIR / \"model_hf_qwen\" / \"qwen2.5-7b\"\n",
    "COMPILED_MODEL_PATH   = BASE_DIR / \"traced_model_qwen2.5\" / \"qwen2.5-7b\" / str(profile[\"tp\"]) / str(profile[\"batch_size\"])\n",
    "ORIGINAL_MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "COMPILED_MODEL_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download",
   "metadata": {},
   "source": [
    "## 3. Model Download\n",
    "\n",
    "Download the pre-trained model from Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model_if_needed(model_id: str, local_dir: Path) -> None:\n",
    "    \"\"\"Download model if not already present locally.\"\"\"\n",
    "    if not (local_dir / \"config.json\").exists():\n",
    "        print(f\"üì• Downloading {model_id} to {local_dir}...\")\n",
    "        snapshot_download(model_id, local_dir=str(local_dir))\n",
    "        print(\"‚úÖ Download complete\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Model already exists at {local_dir}\")\n",
    "\n",
    "download_model_if_needed(MODEL_ID, ORIGINAL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer_setup",
   "metadata": {},
   "source": [
    "## 4. Tokenizer and Generation Configuration\n",
    "\n",
    "Set up the tokenizer and generation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tokenizer_and_generation_config(model_path: Path) -> tuple:\n",
    "    \"\"\"Initialize tokenizer and generation configuration.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(model_path), padding_side=\"right\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    generation_config = GenerationConfig.from_pretrained(str(model_path))\n",
    "    generation_config_kwargs = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 1,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    }\n",
    "    generation_config.update(**generation_config_kwargs)\n",
    "    \n",
    "    print(f\"‚úÖ Tokenizer setup complete. Vocab size: {tokenizer.vocab_size}\")\n",
    "    return tokenizer, generation_config\n",
    "\n",
    "tokenizer, generation_config = setup_tokenizer_and_generation_config(ORIGINAL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neuron_config",
   "metadata": {},
   "source": [
    "## 5. Neuron Configuration\n",
    "\n",
    "This is where we configure Neuron-specific parameters:\n",
    "\n",
    "- **tp_degree**: Tensor parallelism degree (number of Neuron cores to use)\n",
    "- **batch_size**: Number of sequences to process in parallel\n",
    "- **max_context_length**: Maximum input sequence length\n",
    "- **seq_len**: Maximum total sequence length (input + output)\n",
    "- **bucketing**: Pre-compile for different sequence lengths for optimal performance\n",
    "- **on_device_sampling**: Perform sampling on Neuron device for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neuron_config_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuronx_distributed_inference.models.config import NeuronConfig, OnDeviceSamplingConfig\n",
    "\n",
    "def create_neuron_config() -> NeuronConfig:\n",
    "    \"\"\"Create Neuron-specific configuration for optimal performance.\"\"\"\n",
    "    return NeuronConfig(\n",
    "        # Parallelism configuration\n",
    "        tp_degree=TP_DEGREE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        \n",
    "        # Sequence length configuration\n",
    "        max_context_length=1024,  # Maximum input tokens\n",
    "        seq_len=2048,  # Maximum total sequence length\n",
    "        \n",
    "        # Performance optimizations\n",
    "        enable_bucketing=True,  # Enable bucketing for different sequence lengths\n",
    "        context_encoding_buckets=[1024],  # Pre-compile for these context lengths\n",
    "        token_generation_buckets=[2048],  # Pre-compile for these generation lengths\n",
    "        \n",
    "        # Sampling configuration\n",
    "        on_device_sampling_config=OnDeviceSamplingConfig(top_k=5),\n",
    "        \n",
    "        # Model-specific optimizations\n",
    "        flash_decoding_enabled=False,  # Disable for this demo\n",
    "        torch_dtype=torch.bfloat16,  # Use bfloat16 for better performance\n",
    "        attn_kernel_enabled=True,  # Enable optimized attention kernels\n",
    "        attn_cls=\"NeuronQwen2Attention\"  # Use Qwen2-specific attention implementation\n",
    "    )\n",
    "\n",
    "neuron_config = create_neuron_config()\n",
    "print(\"‚úÖ Neuron configuration created\")\n",
    "print(f\"   - Tensor parallelism degree: {neuron_config.tp_degree}\")\n",
    "print(f\"   - Batch size: {neuron_config.batch_size}\")\n",
    "print(f\"   - Max context length: {neuron_config.max_context_length}\")\n",
    "print(f\"   - Sequence length: {neuron_config.seq_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_compilation",
   "metadata": {},
   "source": [
    "## 6. Model Compilation\n",
    "\n",
    "This step converts the PyTorch model to Neuron-optimized format. This is a one-time process that can take 10-30 minutes depending on the model size and configuration.\n",
    "\n",
    "**Note**: Compilation creates optimized compute graphs specifically for your hardware and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compile_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuronx_distributed_inference.models.qwen2.modeling_qwen2 import Qwen2InferenceConfig, NeuronQwen2ForCausalLM\n",
    "from neuronx_distributed_inference.utils.hf_adapter import HuggingFaceGenerationAdapter, load_pretrained_config\n",
    "\n",
    "# üîπ  compile or load flag  üîπ\n",
    "COMPILE_MODEL = 1  # 1 = compile, 0 = only load\n",
    "\n",
    "def compile_or_load(model_path: Path, compiled_path: Path, neuron_cfg: NeuronConfig):\n",
    "    \"\"\"Compile if requested, else only load.\"\"\"\n",
    "\n",
    "    # Safety: inf2.xlarge does not have enough DRAM for compilation\n",
    "    if INSTANCE_TYPE == \"inf2.xlarge\" and COMPILE_MODEL:\n",
    "        raise RuntimeError(\"Compilation on inf2.xlarge is not supported. \"\n",
    "                           \"Set NEURON_COMPILE=0 and use a pre-compiled model.\")\n",
    "\n",
    "    if COMPILE_MODEL:\n",
    "        if (compiled_path / \"pytorch_model.bin\").exists():\n",
    "            print(\"‚ö†Ô∏è  Compiled model already exists ‚Äì skipping compilation.\")\n",
    "        else:\n",
    "            print(\"üî® Compiling model ‚Ä¶ this can take ~30 min.\")\n",
    "            cfg = Qwen2InferenceConfig(\n",
    "                neuron_config,\n",
    "                load_config=load_pretrained_config(str(model_path)),\n",
    "            )\n",
    "            model = NeuronQwen2ForCausalLM(str(model_path), cfg)\n",
    "            model.compile(str(compiled_path))\n",
    "            tokenizer.save_pretrained(str(compiled_path))\n",
    "            \n",
    "            print(\"‚úÖ Compilation finished.\")\n",
    "    else:\n",
    "        print(\"üö´ Compilation skipped (NEURON_COMPILE=0).\")\n",
    "\n",
    "    # --- load compiled artefacts ---\n",
    "    model = NeuronQwen2ForCausalLM(str(compiled_path))\n",
    "    model.load(str(compiled_path))\n",
    "    print(\"‚úÖ Model loaded from disk.\")\n",
    "    return model\n",
    "\n",
    "# run it\n",
    "neuron_model = compile_or_load(ORIGINAL_MODEL_PATH, COMPILED_MODEL_PATH, neuron_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_demo",
   "metadata": {},
   "source": [
    "## 7. Inference Demonstration\n",
    "\n",
    "Now let's run inference with our Neuron-optimized model. We'll demonstrate both regular and \"thinking\" modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuronx_distributed_inference.utils.hf_adapter import HuggingFaceGenerationAdapter\n",
    "\n",
    "def setup_inference_components(model, model_path: Path):\n",
    "    \"\"\"Setup tokenizer and generation adapter for inference.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    generation_config = GenerationConfig.from_pretrained(str(ORIGINAL_MODEL_PATH))\n",
    "    generation_config_kwargs = {\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_k\": 5,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    }\n",
    "    generation_config.update(**generation_config_kwargs)\n",
    "    \n",
    "    generation_model = HuggingFaceGenerationAdapter(model)\n",
    "    \n",
    "    return tokenizer, generation_model\n",
    "\n",
    "def parse_thinking_output(output_ids: list, tokenizer) -> tuple:\n",
    "    \"\"\"Parse thinking content from model output.\"\"\"\n",
    "    try:\n",
    "        # Find the end of thinking token (151668 = </think>)\n",
    "        think_end_token = 151668\n",
    "        index = len(output_ids) - output_ids[::-1].index(think_end_token)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    response_content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    return thinking_content, response_content\n",
    "\n",
    "def run_inference(model, messages: list, enable_thinking: bool = False, max_new_tokens: int = 512):\n",
    "    \"\"\"Run inference with the Neuron model.\"\"\"\n",
    "    tokenizer, generation_model = setup_inference_components(model, COMPILED_MODEL_PATH)\n",
    "    \n",
    "    # Prepare input\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids']\n",
    "    \n",
    "    print(f\"üîÑ Running inference (thinking={'enabled' if enable_thinking else 'disabled'})...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = generation_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Extract generated tokens\n",
    "    output_ids = outputs[0][len(inputs.input_ids[0]):].tolist()\n",
    "    \n",
    "    if enable_thinking:\n",
    "        thinking_content, response_content = parse_thinking_output(output_ids, tokenizer)\n",
    "        return thinking_content, response_content, inference_time\n",
    "    else:\n",
    "        response_content = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "        return None, response_content, inference_time\n",
    "\n",
    "print(\"‚úÖ Inference functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple_inference",
   "metadata": {},
   "source": [
    "### 7.1 Simple Question (No Thinking Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple_question",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple question without thinking mode\n",
    "messages = [{'role': 'user', 'content': \"What's your name?\"}]\n",
    "\n",
    "thinking, response, inference_time = run_inference(\n",
    "    neuron_model, \n",
    "    messages, \n",
    "    enable_thinking=False, \n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Performance: {inference_time:.2f} seconds\")\n",
    "print(f\"\\nü§ñ Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance_comparison",
   "metadata": {},
   "source": [
    "## 8. Performance Analysis\n",
    "\n",
    "Let's run a few more examples to analyze performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_system_resources():\n",
    "    \"\"\"Monitor system resources during inference.\"\"\"\n",
    "    return {\n",
    "        'cpu_percent': psutil.cpu_percent(),\n",
    "        'memory_percent': psutil.virtual_memory().percent,\n",
    "        'available_memory_gb': psutil.virtual_memory().available / (1024**3)\n",
    "    }\n",
    "\n",
    "def benchmark_inference(model, num_runs: int = 3):\n",
    "    \"\"\"Benchmark inference performance.\"\"\"\n",
    "    print(f\"üî¨ Running performance benchmark ({num_runs} runs)...\")\n",
    "    \n",
    "    test_cases = [\n",
    "        {\"messages\": [{'role': 'user', 'content': \"Explain quantum computing in simple terms.\"}], \"max_tokens\": 256},\n",
    "        {\"messages\": [{'role': 'user', 'content': \"Write a short poem about machine learning.\"}], \"max_tokens\": 128},\n",
    "        {\"messages\": [{'role': 'user', 'content': \"What are the benefits of using AWS Neuron?\"}], \"max_tokens\": 200}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        model.reset()\n",
    "        \n",
    "        # Monitor resources before inference\n",
    "        pre_resources = monitor_system_resources()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        _, response, inference_time = run_inference(\n",
    "            model, \n",
    "            test_case[\"messages\"], \n",
    "            enable_thinking=False, \n",
    "            max_new_tokens=test_case[\"max_tokens\"]\n",
    "        )\n",
    "        \n",
    "        # Calculate tokens generated (approximate)\n",
    "        tokens_generated = len(response.split()) * 1.3  # Rough token count\n",
    "        tokens_per_second = tokens_generated / inference_time\n",
    "        \n",
    "        post_resources = monitor_system_resources()\n",
    "        \n",
    "        result = {\n",
    "            'test_case': i + 1,\n",
    "            'inference_time': inference_time,\n",
    "            'tokens_generated': int(tokens_generated),\n",
    "            'tokens_per_second': tokens_per_second,\n",
    "            'cpu_usage': post_resources['cpu_percent'],\n",
    "            'memory_usage': post_resources['memory_percent']\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"   Test {i+1}: {inference_time:.2f}s, {tokens_per_second:.1f} tokens/s\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    avg_time = sum(r['inference_time'] for r in results) / len(results)\n",
    "    avg_tokens_per_sec = sum(r['tokens_per_second'] for r in results) / len(results)\n",
    "    \n",
    "    print(f\"\\nüìä Performance Summary:\")\n",
    "    print(f\"   Average inference time: {avg_time:.2f} seconds\")\n",
    "    print(f\"   Average throughput: {avg_tokens_per_sec:.1f} tokens/second\")\n",
    "    print(f\"   Instance type: {INSTANCE_TYPE}\")\n",
    "    print(f\"   Tensor parallelism: {TP_DEGREE} cores\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "avg_time = benchmark_inference(neuron_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## 10. Production Deployment Guidelines\n",
    "\n",
    "### Compilation Strategy\n",
    "- **Development**: Compile on larger instances (inf2.8xlarge+), then copy artifacts\n",
    "- **Production**: Load pre-compiled models to minimize startup time\n",
    "- **CI/CD**: Include compilation step in your model deployment pipeline\n",
    "\n",
    "### Performance Optimization Tips\n",
    "1. **Right-size your instance**: Start with inf2.xlarge for most workloads for 8B model\n",
    "2. **Optimize sequence lengths**: Use bucketing for variable-length inputs\n",
    "3. **Batch similar requests**: Group requests with similar token counts\n",
    "4. **Monitor utilization**: Use Neuron metrics\n",
    "\n",
    "### Cost Optimization\n",
    "- **Reserved Instances**: For predictable workloads, use Reserved Instances (up to 70% savings)\n",
    "- **Spot Instances**: For fault-tolerant batch processing\n",
    "- **Auto Scaling**: Scale Neuron instances based on request volume\n",
    "\n",
    "### Next Steps\n",
    "- Integrate with your existing inference pipeline (FastAPI, vLLM, etc.)\n",
    "- Set up monitoring with CloudWatch and Neuron Monitor\n",
    "- Consider multi-model serving for better resource utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup_final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "print(\"üßπ Cleaning up resources...\")\n",
    "if 'neuron_model' in locals():\n",
    "    neuron_model.reset()\n",
    "print(\"‚úÖ Demo completed successfully!\")\n",
    "print(f\"\\nüìÅ Compiled model available at: {COMPILED_MODEL_PATH}\")\n",
    "print(\"üí° You can reuse the compiled model for future inference without recompilation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_2_7_nxd_inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
