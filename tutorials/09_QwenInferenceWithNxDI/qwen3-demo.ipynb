{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# AWS Neuron Inference Demo: Qwen3-8B Model\n",
    "\n",
    "## What is AWS Neuron?\n",
    "AWS Neuron is a specialized SDK and runtime for running machine learning inference on **AWS Inferentia** and **Trainium** chips - purpose-built silicon optimized for ML workloads. Unlike general-purpose GPUs, these chips are designed specifically for inference, offering:\n",
    "\n",
    "- **Better cost-performance**: Up to 70% lower cost per inference vs comparable GPU instances\n",
    "- **Predictable performance**: Consistent latency without the variability of shared GPU resources  \n",
    "- **High throughput**: Optimized for batch inference workloads\n",
    "\n",
    "## What is NeuronX Distributed Inference (NxDI)?\n",
    "NxDI is PyTorch-based library that simplifies deploying large language models on Neuron hardware. It provides:\n",
    "- **Production-ready models** (Llama, Qwen, Mixtral, etc.)\n",
    "- **Advanced inference features** (continuous batching, speculative decoding, KV caching)\n",
    "- **Distributed strategies** (tensor parallelism across multiple Neuron cores)\n",
    "- **Seamless integration** with existing PyTorch workflows\n",
    "\n",
    "## Key Concepts You'll Learn:\n",
    "- **Model Compilation**: Converting PyTorch models to Neuron-optimized format (one-time process)\n",
    "- **Tensor Parallelism**: Splitting model layers across multiple Neuron cores for larger models\n",
    "- **Bucketing**: Pre-compiling for different sequence lengths to avoid recompilation\n",
    "- **On-device Sampling**: Performing text generation sampling directly on Neuron hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported_models",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Supported Qwen3 Models\n",
    "\n",
    "**IMPORTANT**: As of now NxDi supports only the following official Qwen3 model checkpoints:\n",
    "\n",
    "- [Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)\n",
    "- [Qwen/Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B) \n",
    "- [Qwen/Qwen3-4B](https://huggingface.co/Qwen/Qwen3-4B)\n",
    "- [Qwen/Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B) ‚úÖ *Used in this demo*\n",
    "- [Qwen/Qwen3-14B](https://huggingface.co/Qwen/Qwen3-14B)\n",
    "- [Qwen/Qwen3-32B](https://huggingface.co/Qwen/Qwen3-32B)\n",
    "\n",
    "**Note**: \n",
    "- Other Qwen3 variants, fine-tuned models, or custom checkpoints may not be compatible with NeuronX Distributed Inference\n",
    "- For larger models (14B, 32B), you'll need instances with more Neuron cores (inf2.24xlarge or inf2.48xlarge)\n",
    "- This demo uses **Qwen3-8B** as it provides a good balance of capability and resource requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports\n",
    "\n",
    "First, we'll set up the environment and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neuron environment validated\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download, login\n",
    "\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "\n",
    "def validate_environment():\n",
    "    \"\"\"Validate that we're running on a Neuron-enabled instance.\"\"\"\n",
    "    try:\n",
    "        import torch_neuronx\n",
    "        import neuronx_distributed_inference\n",
    "        print(\"‚úÖ Neuron environment validated\")\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Neuron environment not found: {e}\")\n",
    "        print(\"üí° Make sure you're running on an inf2/trn1 instance with Neuron SDK installed\")\n",
    "        return False\n",
    "\n",
    "if not validate_environment():\n",
    "    raise RuntimeError(\"Please run this notebook on a Neuron-enabled instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## 2. Instance Configuration and Neuron Concepts\n",
    "\n",
    "### Understanding Neuron Cores\n",
    "Each AWS Inferentia/Trainium instance contains multiple **Neuron cores** - the compute units that execute your model:\n",
    "- **inf2.xlarge**: 2 cores (good for development/testing)\n",
    "- **inf2.8xlarge**: 2 cores (cost-effective production)  \n",
    "- **inf2.24xlarge**: 12 cores (high-throughput production)\n",
    "- **inf2.48xlarge**: 24 cores (maximum single-instance performance)\n",
    "\n",
    "### Tensor Parallelism (TP)\n",
    "For models too large for a single core, we split them across multiple cores using **tensor parallelism**:\n",
    "- TP degree = number of cores to use\n",
    "- Higher TP = can run larger models, but with communication overhead\n",
    "- For Qwen3-8B: TP=2 is optimal balance of performance and resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "paths",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Target instance : trn1.32xlarge (Neuron cores=32, tp=32, batch_size=16)\n"
     ]
    }
   ],
   "source": [
    "# üîπ  INSTANCE SELECTION  üîπ\n",
    "# ---------------------------------------------------------------------\n",
    "# Supported instances and their Neuron-core counts\n",
    "INSTANCE_PROFILES = {\n",
    "    \"inf2.xlarge\"   : dict(cores=2 , tp=2 , batch_size=1),\n",
    "    \"inf2.8xlarge\"  : dict(cores=2, tp=2 , batch_size=1),\n",
    "    \"inf2.24xlarge\" : dict(cores=12, tp=12 , batch_size=4),\n",
    "    \"inf2.48xlarge\" : dict(cores=24, tp=24 , batch_size=8),\n",
    "    \"trn1.32xlarge\" : dict(cores=32, tp=32 , batch_size=16),\n",
    "}\n",
    "\n",
    "# Choose your target instance here (or via environment variable)\n",
    "INSTANCE_TYPE   = \"trn1.32xlarge\"\n",
    "assert INSTANCE_TYPE in INSTANCE_PROFILES, f\"Unsupported instance {INSTANCE_TYPE}\"\n",
    "\n",
    "profile         = INSTANCE_PROFILES[INSTANCE_TYPE]\n",
    "NUM_CORES       = profile[\"cores\"]\n",
    "TP_DEGREE       = profile[\"tp\"]\n",
    "BATCH_SIZE      = profile[\"batch_size\"]\n",
    "\n",
    "print(f\"üñ•Ô∏è  Target instance : {INSTANCE_TYPE} \"\n",
    "      f\"(Neuron cores={NUM_CORES}, tp={TP_DEGREE}, batch_size={BATCH_SIZE})\")\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# Paths ----------------------------------------------------------------\n",
    "MODEL_ID              = \"Qwen/Qwen3-8B\"\n",
    "BASE_DIR              = Path(\"/home/ubuntu\")\n",
    "ORIGINAL_MODEL_PATH   = BASE_DIR / \"model_hf_qwen\" / \"qwen3-8b\"\n",
    "COMPILED_MODEL_PATH   = BASE_DIR / \"traced_model_qwen3\" / \"qwen3-8b\" / str(profile[\"tp\"]) / str(profile[\"batch_size\"])\n",
    "ORIGINAL_MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "COMPILED_MODEL_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download",
   "metadata": {},
   "source": [
    "## 3. Model Download\n",
    "\n",
    "Download the pre-trained model from Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "download_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading Qwen/Qwen3-8B to /home/ubuntu/model_hf_qwen/qwen3-8b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d258f14dac5a407ab9ec69b288ecd83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3150449f7dc46c9bf0aa65a6c88b20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af31a161d17449285d8db8358d67648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85b50f5a9354f718b4e489f06cbd3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f55ff15c92496db84c90af9e626540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2458514a79ce4bf58fc3f1318ea42bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac1e10228a341a8aa425227f28a21ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65774ab707694f018cf0cb121d3d64d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340978dfdbb146049f2d4e7654a2a894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87db6225a01485e936e7dfe88acbde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85636a60bc8f4b21876d73f87ef47c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800c27d3dd614398ace28ba8b80f129a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb97f9d3e244c8884cf8a8164e1da0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ebc20c9e2e43a68d1c855a216e1660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71295a4f96743ef8619e1e9c99d4416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7ade4f1796416b95385e4256a49a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Download complete\n"
     ]
    }
   ],
   "source": [
    "def download_model_if_needed(model_id: str, local_dir: Path) -> None:\n",
    "    \"\"\"Download model if not already present locally.\"\"\"\n",
    "    if not (local_dir / \"config.json\").exists():\n",
    "        print(f\"üì• Downloading {model_id} to {local_dir}...\")\n",
    "        snapshot_download(model_id, local_dir=str(local_dir))\n",
    "        print(\"‚úÖ Download complete\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Model already exists at {local_dir}\")\n",
    "\n",
    "download_model_if_needed(MODEL_ID, ORIGINAL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer_setup",
   "metadata": {},
   "source": [
    "## 4. Tokenizer and Generation Configuration\n",
    "\n",
    "Set up the tokenizer and generation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "setup_tokenizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer setup complete. Vocab size: 151643\n"
     ]
    }
   ],
   "source": [
    "def setup_tokenizer_and_generation_config(model_path: Path) -> tuple:\n",
    "    \"\"\"Initialize tokenizer and generation configuration.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(model_path), padding_side=\"right\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    generation_config = GenerationConfig.from_pretrained(str(model_path))\n",
    "    generation_config_kwargs = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 1,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    }\n",
    "    generation_config.update(**generation_config_kwargs)\n",
    "    \n",
    "    print(f\"‚úÖ Tokenizer setup complete. Vocab size: {tokenizer.vocab_size}\")\n",
    "    return tokenizer, generation_config\n",
    "\n",
    "tokenizer, generation_config = setup_tokenizer_and_generation_config(ORIGINAL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neuron_config",
   "metadata": {},
   "source": [
    "## 5. Neuron Configuration\n",
    "\n",
    "This is where we configure Neuron-specific parameters:\n",
    "\n",
    "- **tp_degree**: Tensor parallelism degree (number of Neuron cores to use)\n",
    "- **batch_size**: Number of sequences to process in parallel\n",
    "- **max_context_length**: Maximum input sequence length\n",
    "- **seq_len**: Maximum total sequence length (input + output)\n",
    "- **bucketing**: Pre-compile for different sequence lengths for optimal performance\n",
    "- **on_device_sampling**: Perform sampling on Neuron device for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "neuron_config_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neuron configuration created\n",
      "   - Tensor parallelism degree: 32\n",
      "   - Batch size: 16\n",
      "   - Max context length: 1024\n",
      "   - Sequence length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/attention/utils.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.custom_calls import neuron_cumsum\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n"
     ]
    }
   ],
   "source": [
    "from neuronx_distributed_inference.models.config import NeuronConfig, OnDeviceSamplingConfig\n",
    "\n",
    "def create_neuron_config() -> NeuronConfig:\n",
    "    \"\"\"Create Neuron-specific configuration for optimal performance.\"\"\"\n",
    "    return NeuronConfig(\n",
    "        # Parallelism configuration\n",
    "        tp_degree=TP_DEGREE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        \n",
    "        # Sequence length configuration\n",
    "        max_context_length=1024,  # Maximum input tokens\n",
    "        seq_len=2048,  # Maximum total sequence length\n",
    "        \n",
    "        # Performance optimizations\n",
    "        enable_bucketing=True,  # Enable bucketing for different sequence lengths\n",
    "        context_encoding_buckets=[1024],  # Pre-compile for these context lengths\n",
    "        token_generation_buckets=[2048],  # Pre-compile for these generation lengths\n",
    "        \n",
    "        # Sampling configuration\n",
    "        on_device_sampling_config=OnDeviceSamplingConfig(top_k=5),\n",
    "        \n",
    "        # Model-specific optimizations\n",
    "        flash_decoding_enabled=False,  # Disable for this demo\n",
    "        torch_dtype=torch.bfloat16,  # Use bfloat16 for better performance\n",
    "        attn_kernel_enabled=True,  # Enable optimized attention kernels\n",
    "        attn_cls=\"NeuronQwen3Attention\"  # Use Qwen3-specific attention implementation\n",
    "    )\n",
    "\n",
    "neuron_config = create_neuron_config()\n",
    "print(\"‚úÖ Neuron configuration created\")\n",
    "print(f\"   - Tensor parallelism degree: {neuron_config.tp_degree}\")\n",
    "print(f\"   - Batch size: {neuron_config.batch_size}\")\n",
    "print(f\"   - Max context length: {neuron_config.max_context_length}\")\n",
    "print(f\"   - Sequence length: {neuron_config.seq_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_compilation",
   "metadata": {},
   "source": [
    "## 6. Model Compilation\n",
    "\n",
    "This step converts the PyTorch model to Neuron-optimized format. This is a one-time process that can take 10-30 minutes depending on the model size and configuration.\n",
    "\n",
    "**Note**: Compilation creates optimized compute graphs specifically for your hardware and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "compile_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/models/llama/modeling_llama.py:63: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/models/llama/modeling_llama.py:63: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/models/llama/modeling_llama.py:63: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase\n",
      "Neuron: Saving the neuron_config to /home/ubuntu/traced_model_qwen3/qwen3-8b/32/16/\n",
      "Neuron: Generating HLOs for the following models: ['context_encoding_model', 'token_generation_model']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî® Compiling model ‚Ä¶ this can take ~30 min.\n",
      "[2025-09-02 07:46:38.169: I neuronx_distributed/parallel_layers/parallel_state.py:628] > initializing tensor model parallel with size 32\n",
      "[2025-09-02 07:46:38.170: I neuronx_distributed/parallel_layers/parallel_state.py:629] > initializing pipeline model parallel with size 1\n",
      "[2025-09-02 07:46:38.170: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing context model parallel with size 1\n",
      "[2025-09-02 07:46:38.171: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing data parallel with size 1\n",
      "[2025-09-02 07:46:38.172: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing world size to 32\n",
      "[2025-09-02 07:46:38.173: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x72ac063539a0>, 'Ascending Ring PG Group')>\n",
      "[2025-09-02 07:46:38.175: I neuronx_distributed/parallel_layers/parallel_state.py:668] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]\n",
      "[2025-09-02 07:46:38.175: I neuronx_distributed/parallel_layers/parallel_state.py:669] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\n",
      "[2025-09-02 07:46:38.176: I neuronx_distributed/parallel_layers/parallel_state.py:670] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\n",
      "[2025-09-02 07:46:38.176: I neuronx_distributed/parallel_layers/parallel_state.py:671] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\n",
      "[2025-09-02 07:46:38.177: I neuronx_distributed/parallel_layers/parallel_state.py:672] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\n",
      "[2025-09-02 07:46:38.177: I neuronx_distributed/parallel_layers/parallel_state.py:673] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Generating 1 hlos for key: context_encoding_model\n",
      "Neuron: Started loading module context_encoding_model\n",
      "Neuron: Finished loading module context_encoding_model in 0.08832788467407227 seconds\n",
      "Neuron: generating HLO: context_encoding_model, input example shape = torch.Size([16, 1024])\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([16, 1024]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([16]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=4, shape=torch.Size([16, 3]), dtype=torch.float32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=5, shape=torch.Size([16]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=6, shape=torch.Size([16]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "Neuron: Finished generating HLO for context_encoding_model in 7.0562803745269775 seconds, input example shape = torch.Size([16, 1024])\n",
      "Neuron: Generating 1 hlos for key: token_generation_model\n",
      "Neuron: Started loading module token_generation_model\n",
      "Neuron: Finished loading module token_generation_model in 0.0668025016784668 seconds\n",
      "Neuron: generating HLO: token_generation_model, input example shape = torch.Size([16, 1])\n",
      "Neuron: Finished generating HLO for token_generation_model in 1.0833957195281982 seconds, input example shape = torch.Size([16, 1])\n",
      "Neuron: Generated all HLOs in 8.371405601501465 seconds\n",
      "Neuron: Starting compilation for the priority HLO\n",
      "Neuron: 'token_generation_model' is the priority model with bucket rank 0\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-02 07:46:47.000464:  12221  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxd_model/token_generation_model/_tp0_bk0/model.MODULE_c4f0c212f54294e84e33+617f6939.hlo_module.pb --output /tmp/nxd_model/token_generation_model/_tp0_bk0/model.MODULE_c4f0c212f54294e84e33+617f6939.neff --target=trn1 --auto-cast=none --model-type=transformer --tensorizer-options=--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma  --lnc=1 -O2 --internal-hlo2tensorizer-options=--verify-hlo=true --logfile=/tmp/nxd_model/token_generation_model/_tp0_bk0/log-neuron-cc.txt --enable-internal-neff-wrapper --verbose=35\n",
      "...........Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Done compilation for the priority HLO in 216.08494329452515 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Updating the hlo module with optimized layout\n",
      "Neuron: Done optimizing weight layout for all HLOs in 4.648864030838013 seconds\n",
      "Neuron: Starting compilation for all HLOs\n",
      "Neuron: Neuron compiler flags: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=1 -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true'  --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk0/log-neuron-cc.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-02 07:50:27.000396:  12221  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxd_model/context_encoding_model/_tp0_bk0/model.MODULE_6020fd81e9865b09a888+ad9e832d.hlo_module.pb --output /tmp/nxd_model/context_encoding_model/_tp0_bk0/model.MODULE_6020fd81e9865b09a888+ad9e832d.neff --target=trn1 --auto-cast=none --model-type=transformer --tensorizer-options=--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma  --lnc=1 -O1 --internal-hlo2tensorizer-options= --modular-flow-mac-threshold=10  --verify-hlo=true --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk0/log-neuron-cc.txt --verbose=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:245: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..Completed run_backend_driver.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Finished Compilation for all HLOs in 38.93697476387024 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiler status PASS\n",
      ".."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Done preparing weight layout transformation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Finished building model in 295.1843423843384 seconds\n",
      "Neuron: SKIPPING pre-sharding the checkpoints. The checkpoints will be sharded during load time.\n",
      "root: NeuronConfig init: Unexpected keyword arguments: {'apply_seq_ids_mask': False, 'enable_long_context_mode': False, 'enable_output_completion_notifications': False, 'enable_token_tree': False, 'is_chunked_prefill': False, 'is_prefill_stage': None, 'kv_cache_tiling': False, 'scratchpad_page_size': None, 'skip_warmup': False, 'tile_cc': False, 'weights_to_skip_layout_optimization': []}\n",
      "Neuron: Sharding weights on load...\n",
      "Neuron: Sharding Weights for ranks: 0...31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Compilation finished.\n",
      "[2025-09-02 07:51:33.550: I neuronx_distributed/parallel_layers/parallel_state.py:628] > initializing tensor model parallel with size 32\n",
      "[2025-09-02 07:51:33.551: I neuronx_distributed/parallel_layers/parallel_state.py:629] > initializing pipeline model parallel with size 1\n",
      "[2025-09-02 07:51:33.551: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing context model parallel with size 1\n",
      "[2025-09-02 07:51:33.552: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing data parallel with size 1\n",
      "[2025-09-02 07:51:33.552: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing world size to 32\n",
      "[2025-09-02 07:51:33.553: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x72ac063539a0>, 'Ascending Ring PG Group')>\n",
      "[2025-09-02 07:51:33.555: I neuronx_distributed/parallel_layers/parallel_state.py:668] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]\n",
      "[2025-09-02 07:51:33.556: I neuronx_distributed/parallel_layers/parallel_state.py:669] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\n",
      "[2025-09-02 07:51:33.556: I neuronx_distributed/parallel_layers/parallel_state.py:670] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\n",
      "[2025-09-02 07:51:33.556: I neuronx_distributed/parallel_layers/parallel_state.py:671] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\n",
      "[2025-09-02 07:51:33.557: I neuronx_distributed/parallel_layers/parallel_state.py:672] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\n",
      "[2025-09-02 07:51:33.557: I neuronx_distributed/parallel_layers/parallel_state.py:673] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/trace/trace.py:640: UserWarning: Removing redundant keys from checkpoint: ['layers.0.self_attn.k_norm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_norm.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.self_attn.k_norm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_norm.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.2.self_attn.k_norm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_norm.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.3.self_attn.k_norm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_norm.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.self_attn.k_norm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_norm.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.self_attn.k_norm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_norm.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.self_attn.k_norm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_norm.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.10.self_attn.k_norm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_norm.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.self_attn.k_norm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_norm.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.self_attn.k_norm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_norm.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.self_attn.k_norm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_norm.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.self_attn.k_norm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_norm.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.self_attn.k_norm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_norm.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.16.self_attn.k_norm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_norm.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.self_attn.k_norm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_norm.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.7.self_attn.k_norm.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_norm.weight', 'layers.8.self_attn.k_norm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_norm.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.self_attn.k_norm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_norm.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'layers.18.self_attn.k_norm.weight', 'layers.18.self_attn.k_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.self_attn.q_norm.weight', 'layers.18.self_attn.q_proj.weight', 'layers.18.self_attn.v_proj.weight', 'layers.19.self_attn.k_norm.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.q_norm.weight', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.20.self_attn.k_norm.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.q_norm.weight', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.self_attn.k_norm.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.q_norm.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.22.self_attn.k_norm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.q_norm.weight', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.23.self_attn.k_norm.weight', 'layers.23.self_attn.k_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.23.self_attn.q_norm.weight', 'layers.23.self_attn.q_proj.weight', 'layers.23.self_attn.v_proj.weight', 'layers.24.self_attn.k_norm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.q_norm.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.25.self_attn.k_norm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.q_norm.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.26.self_attn.k_norm.weight', 'layers.26.self_attn.k_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.26.self_attn.q_norm.weight', 'layers.26.self_attn.q_proj.weight', 'layers.26.self_attn.v_proj.weight', 'layers.27.self_attn.k_norm.weight', 'layers.27.self_attn.k_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.q_norm.weight', 'layers.27.self_attn.q_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.28.self_attn.k_norm.weight', 'layers.28.self_attn.k_proj.weight', 'layers.28.self_attn.o_proj.weight', 'layers.28.self_attn.q_norm.weight', 'layers.28.self_attn.q_proj.weight', 'layers.28.self_attn.v_proj.weight', 'layers.29.self_attn.k_norm.weight', 'layers.29.self_attn.k_proj.weight', 'layers.29.self_attn.o_proj.weight', 'layers.29.self_attn.q_norm.weight', 'layers.29.self_attn.q_proj.weight', 'layers.29.self_attn.v_proj.weight', 'layers.30.self_attn.k_norm.weight', 'layers.30.self_attn.k_proj.weight', 'layers.30.self_attn.o_proj.weight', 'layers.30.self_attn.q_norm.weight', 'layers.30.self_attn.q_proj.weight', 'layers.30.self_attn.v_proj.weight', 'layers.31.self_attn.k_norm.weight', 'layers.31.self_attn.k_proj.weight', 'layers.31.self_attn.o_proj.weight', 'layers.31.self_attn.q_norm.weight', 'layers.31.self_attn.q_proj.weight', 'layers.31.self_attn.v_proj.weight', 'layers.32.self_attn.k_norm.weight', 'layers.32.self_attn.k_proj.weight', 'layers.32.self_attn.o_proj.weight', 'layers.32.self_attn.q_norm.weight', 'layers.32.self_attn.q_proj.weight', 'layers.32.self_attn.v_proj.weight', 'layers.33.self_attn.k_norm.weight', 'layers.33.self_attn.k_proj.weight', 'layers.33.self_attn.o_proj.weight', 'layers.33.self_attn.q_norm.weight', 'layers.33.self_attn.q_proj.weight', 'layers.33.self_attn.v_proj.weight', 'layers.34.self_attn.k_norm.weight', 'layers.34.self_attn.k_proj.weight', 'layers.34.self_attn.o_proj.weight', 'layers.34.self_attn.q_norm.weight', 'layers.34.self_attn.q_proj.weight', 'layers.34.self_attn.v_proj.weight', 'layers.35.self_attn.k_norm.weight', 'layers.35.self_attn.k_proj.weight', 'layers.35.self_attn.o_proj.weight', 'layers.35.self_attn.q_norm.weight', 'layers.35.self_attn.q_proj.weight', 'layers.35.self_attn.v_proj.weight']\n",
      "  warnings.warn(f\"Removing redundant keys from checkpoint: {keys_to_delete}\")\n",
      "Neuron: Done Sharding weights in 2.1348061939997933\n",
      "Neuron: Finished weights loading in 44.940252185999725 seconds\n",
      "Neuron: Warming up the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-Sep-02 07:52:19.0001 12221:14033 [8] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):213 CCOM WARN NET/OFI Failed to initialize sendrecv protocol\n",
      "2025-Sep-02 07:52:19.0005 12221:14033 [8] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):354 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2025-Sep-02 07:52:19.0010 12221:14033 [8] ncclResult_t nccl_net_ofi_init_no_atexit_fini_v6(ncclDebugLogger_t):183 CCOM WARN NET/OFI Initializing plugin failed\n",
      "2025-Sep-02 07:52:19.0015 12221:14033 [8] net_plugin.cc:97 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Warmup completed in 2.6343612670898438 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "from neuronx_distributed_inference.models.qwen3.modeling_qwen3 import Qwen3InferenceConfig, NeuronQwen3ForCausalLM\n",
    "from neuronx_distributed_inference.utils.hf_adapter import HuggingFaceGenerationAdapter, load_pretrained_config\n",
    "\n",
    "# üîπ  compile or load flag  üîπ\n",
    "COMPILE_MODEL = 1  # 1 = compile, 0 = only load\n",
    "\n",
    "def compile_or_load(model_path: Path, compiled_path: Path, neuron_cfg: NeuronConfig):\n",
    "    \"\"\"Compile if requested, else only load.\"\"\"\n",
    "\n",
    "    # Safety: inf2.xlarge does not have enough DRAM for compilation\n",
    "    if INSTANCE_TYPE == \"inf2.xlarge\" and COMPILE_MODEL:\n",
    "        raise RuntimeError(\"Compilation on inf2.xlarge is not supported. \"\n",
    "                           \"Set NEURON_COMPILE=0 and use a pre-compiled model.\")\n",
    "\n",
    "    if COMPILE_MODEL:\n",
    "        if (compiled_path / \"pytorch_model.bin\").exists():\n",
    "            print(\"‚ö†Ô∏è  Compiled model already exists ‚Äì skipping compilation.\")\n",
    "        else:\n",
    "            print(\"üî® Compiling model ‚Ä¶ this can take ~30 min.\")\n",
    "            cfg = Qwen3InferenceConfig(\n",
    "                neuron_config,\n",
    "                load_config=load_pretrained_config(str(model_path)),\n",
    "            )\n",
    "            model = NeuronQwen3ForCausalLM(str(model_path), cfg)\n",
    "            model.compile(str(compiled_path))\n",
    "            tokenizer.save_pretrained(str(compiled_path))\n",
    "            \n",
    "            print(\"‚úÖ Compilation finished.\")\n",
    "    else:\n",
    "        print(\"üö´ Compilation skipped (NEURON_COMPILE=0).\")\n",
    "\n",
    "    # --- load compiled artefacts ---\n",
    "    model = NeuronQwen3ForCausalLM(str(compiled_path))\n",
    "    model.load(str(compiled_path))\n",
    "    print(\"‚úÖ Model loaded from disk.\")\n",
    "    return model\n",
    "\n",
    "# run it\n",
    "neuron_model = compile_or_load(ORIGINAL_MODEL_PATH, COMPILED_MODEL_PATH, neuron_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_demo",
   "metadata": {},
   "source": [
    "## 7. Inference Demonstration\n",
    "\n",
    "Now let's run inference with our Neuron-optimized model. We'll demonstrate both regular and \"thinking\" modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "inference_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference functions ready\n"
     ]
    }
   ],
   "source": [
    "from neuronx_distributed_inference.utils.hf_adapter import HuggingFaceGenerationAdapter\n",
    "\n",
    "def setup_inference_components(model, model_path: Path):\n",
    "    \"\"\"Setup tokenizer and generation adapter for inference.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    generation_config = GenerationConfig.from_pretrained(str(ORIGINAL_MODEL_PATH))\n",
    "    generation_config_kwargs = {\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_k\": 5,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    }\n",
    "    generation_config.update(**generation_config_kwargs)\n",
    "    \n",
    "    generation_model = HuggingFaceGenerationAdapter(model)\n",
    "    \n",
    "    return tokenizer, generation_model\n",
    "\n",
    "def parse_thinking_output(output_ids: list, tokenizer) -> tuple:\n",
    "    \"\"\"Parse thinking content from model output.\"\"\"\n",
    "    try:\n",
    "        # Find the end of thinking token (151668 = </think>)\n",
    "        think_end_token = 151668\n",
    "        index = len(output_ids) - output_ids[::-1].index(think_end_token)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    response_content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    return thinking_content, response_content\n",
    "\n",
    "def run_inference(model, messages: list, enable_thinking: bool = False, max_new_tokens: int = 512):\n",
    "    \"\"\"Run inference with the Neuron model.\"\"\"\n",
    "    tokenizer, generation_model = setup_inference_components(model, COMPILED_MODEL_PATH)\n",
    "    \n",
    "    # Prepare input\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids']\n",
    "    \n",
    "    print(f\"üîÑ Running inference (thinking={'enabled' if enable_thinking else 'disabled'})...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = generation_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Extract generated tokens\n",
    "    output_ids = outputs[0][len(inputs.input_ids[0]):].tolist()\n",
    "    \n",
    "    if enable_thinking:\n",
    "        thinking_content, response_content = parse_thinking_output(output_ids, tokenizer)\n",
    "        return thinking_content, response_content, inference_time\n",
    "    else:\n",
    "        response_content = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "        return None, response_content, inference_time\n",
    "\n",
    "print(\"‚úÖ Inference functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple_inference",
   "metadata": {},
   "source": [
    "### 7.1 Simple Question (No Thinking Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "simple_question",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "HuggingFaceGenerationAdapter has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running inference (thinking=disabled)...\n",
      "\n",
      "üìä Performance: 1.35 seconds\n",
      "\n",
      "ü§ñ Response: My name is Qwen, and I'm a large language model developed by Alibaba Cloud. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Simple question without thinking mode\n",
    "messages = [{'role': 'user', 'content': \"What's your name?\"}]\n",
    "\n",
    "thinking, response, inference_time = run_inference(\n",
    "    neuron_model, \n",
    "    messages, \n",
    "    enable_thinking=False, \n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Performance: {inference_time:.2f} seconds\")\n",
    "print(f\"\\nü§ñ Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance_comparison",
   "metadata": {},
   "source": [
    "## 8. Performance Analysis\n",
    "\n",
    "Let's run a few more examples to analyze performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "performance_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Running performance benchmark (3 runs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running inference (thinking=disabled)...\n",
      "   Test 1: 2.94s, 83.2 tokens/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running inference (thinking=disabled)...\n",
      "   Test 2: 1.75s, 70.0 tokens/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running inference (thinking=disabled)...\n",
      "   Test 3: 2.40s, 77.6 tokens/s\n",
      "\n",
      "üìä Performance Summary:\n",
      "   Average inference time: 2.36 seconds\n",
      "   Average throughput: 76.9 tokens/second\n",
      "   Instance type: trn1.32xlarge\n",
      "   Tensor parallelism: 32 cores\n"
     ]
    }
   ],
   "source": [
    "def monitor_system_resources():\n",
    "    \"\"\"Monitor system resources during inference.\"\"\"\n",
    "    return {\n",
    "        'cpu_percent': psutil.cpu_percent(),\n",
    "        'memory_percent': psutil.virtual_memory().percent,\n",
    "        'available_memory_gb': psutil.virtual_memory().available / (1024**3)\n",
    "    }\n",
    "\n",
    "def benchmark_inference(model, num_runs: int = 3):\n",
    "    \"\"\"Benchmark inference performance.\"\"\"\n",
    "    print(f\"üî¨ Running performance benchmark ({num_runs} runs)...\")\n",
    "    \n",
    "    test_cases = [\n",
    "        {\"messages\": [{'role': 'user', 'content': \"Explain quantum computing in simple terms.\"}], \"max_tokens\": 256},\n",
    "        {\"messages\": [{'role': 'user', 'content': \"Write a short poem about machine learning.\"}], \"max_tokens\": 128},\n",
    "        {\"messages\": [{'role': 'user', 'content': \"What are the benefits of using AWS Neuron?\"}], \"max_tokens\": 200}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        model.reset()\n",
    "        \n",
    "        # Monitor resources before inference\n",
    "        pre_resources = monitor_system_resources()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        _, response, inference_time = run_inference(\n",
    "            model, \n",
    "            test_case[\"messages\"], \n",
    "            enable_thinking=False, \n",
    "            max_new_tokens=test_case[\"max_tokens\"]\n",
    "        )\n",
    "        \n",
    "        # Calculate tokens generated (approximate)\n",
    "        tokens_generated = len(response.split()) * 1.3  # Rough token count\n",
    "        tokens_per_second = tokens_generated / inference_time\n",
    "        \n",
    "        post_resources = monitor_system_resources()\n",
    "        \n",
    "        result = {\n",
    "            'test_case': i + 1,\n",
    "            'inference_time': inference_time,\n",
    "            'tokens_generated': int(tokens_generated),\n",
    "            'tokens_per_second': tokens_per_second,\n",
    "            'cpu_usage': post_resources['cpu_percent'],\n",
    "            'memory_usage': post_resources['memory_percent']\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"   Test {i+1}: {inference_time:.2f}s, {tokens_per_second:.1f} tokens/s\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    avg_time = sum(r['inference_time'] for r in results) / len(results)\n",
    "    avg_tokens_per_sec = sum(r['tokens_per_second'] for r in results) / len(results)\n",
    "    \n",
    "    print(f\"\\nüìä Performance Summary:\")\n",
    "    print(f\"   Average inference time: {avg_time:.2f} seconds\")\n",
    "    print(f\"   Average throughput: {avg_tokens_per_sec:.1f} tokens/second\")\n",
    "    print(f\"   Instance type: {INSTANCE_TYPE}\")\n",
    "    print(f\"   Tensor parallelism: {TP_DEGREE} cores\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "avg_time = benchmark_inference(neuron_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## 10. Production Deployment Guidelines\n",
    "\n",
    "### Compilation Strategy\n",
    "- **Development**: Compile on larger instances (inf2.8xlarge+), then copy artifacts\n",
    "- **Production**: Load pre-compiled models to minimize startup time\n",
    "- **CI/CD**: Include compilation step in your model deployment pipeline\n",
    "\n",
    "### Performance Optimization Tips\n",
    "1. **Right-size your instance**: Start with inf2.8xlarge for most workloads for 8B model\n",
    "2. **Optimize sequence lengths**: Use bucketing for variable-length inputs\n",
    "3. **Batch similar requests**: Group requests with similar token counts\n",
    "4. **Monitor utilization**: Use Neuron metrics\n",
    "\n",
    "### Cost Optimization\n",
    "- **Reserved Instances**: For predictable workloads, use Reserved Instances (up to 70% savings)\n",
    "- **Spot Instances**: For fault-tolerant batch processing\n",
    "- **Auto Scaling**: Scale Neuron instances based on request volume\n",
    "\n",
    "### Next Steps\n",
    "- Integrate with your existing inference pipeline (FastAPI, vLLM, etc.)\n",
    "- Set up monitoring with CloudWatch and Neuron Monitor\n",
    "- Consider multi-model serving for better resource utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cleanup_final",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up resources...\n",
      "‚úÖ Demo completed successfully!\n",
      "\n",
      "üìÅ Compiled model available at: /home/ubuntu/traced_model_qwen3/qwen3-8b/32/16\n",
      "üí° You can reuse the compiled model for future inference without recompilation.\n"
     ]
    }
   ],
   "source": [
    "# Final cleanup\n",
    "print(\"üßπ Cleaning up resources...\")\n",
    "if 'neuron_model' in locals():\n",
    "    neuron_model.reset()\n",
    "print(\"‚úÖ Demo completed successfully!\")\n",
    "print(f\"\\nüìÅ Compiled model available at: {COMPILED_MODEL_PATH}\")\n",
    "print(\"üí° You can reuse the compiled model for future inference without recompilation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7643f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
