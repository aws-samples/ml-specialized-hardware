{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6cb03f7",
   "metadata": {},
   "source": [
    "# FAQ Bot - Q&A model, trained using pairs of questions and answers\n",
    "\n",
    "Fine tune a large language model with a list of question and answers. This approach os called Closed Book Q&A because the model doesn't require context and is capable of answering variations of the questions you provide in your dataset.\n",
    "\n",
    "This is an evolution of classic ChatBots because LLMs like T5 can disambiguate and generalize better than the old technologies we find in these ChatBots services.\n",
    "\n",
    "For that purpose you'll use a **[T5 SMALL SSM ~80MParams](https://huggingface.co/google/t5-small-ssm)** model, accelerated by a trn1 instance ([AWS Trainium](https://aws.amazon.com/machine-learning/trainium/)), running on [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "\n",
    "You can set the hyperperameter **--model_name** to change the model size. This solution works well with:  \n",
    "  - t5-small-ssm\n",
    "  - t5-large-ssm\n",
    "  \n",
    "If you need to fine tune **t5-3b-ssm, t5-11b-ssm or t5-xxl-ssm**, you need **FSDP**, which is out of the scope of this tutorial.\n",
    "\n",
    "You can see the results of the predictions at the end of this notebook. You'll notice the questions sent to the model are not in the training dataset. They are just variations of the questions used to fine tune the model.\n",
    "\n",
    "The dataset is the content of all **AWS FAQ** pages, downloaded from: https://aws.amazon.com/faqs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb78690",
   "metadata": {},
   "source": [
    "## 1) Install some dependencies and check permissions\n",
    "You need a more recent version of **SageMaker** Python Library. After this install you'll need to restart the kernel.\n",
    "\n",
    "In addition you'll be using [SageMaker Studio Image Build](https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/) to deploy your image. If you have not previously used it please follow that link and setup the required IAM permissions for your SageMaker role.\n",
    "\n",
    ">**If you have not setup IAM permissions to the ECR repositiory in `image_uri`, do so now to prevent errors.**\n",
    "\n",
    ">**If you have never before done a SageMaker training job with Trn1, you'll need to do a service level request. This can take a few hours, best to make the request early so you don't have to wait.**\n",
    "\n",
    "You can edit this URL to go directly to the page to request the increase:\n",
    "\n",
    "`https://<region>.console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas/L-79A1FE57`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ae05a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker-studio-image-build\n",
    "%pip install -U --force-reinstall sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a951ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region=sess.boto_region_name\n",
    "\n",
    "# Make changes here to use an existing repository and/or tag\n",
    "repo_name=\"sagemaker-studio-pytorch-training-neuron\"\n",
    "image_tag=\"1.13.1-neuron-py38-sdk2.9.0-ubuntu20.04\"\n",
    "\n",
    "image_name=f\"{repo_name}:{image_tag}\"\n",
    "image_uri=f\"{sess.account_id()}.dkr.ecr.{region}.amazonaws.com/{repo_name}:{image_tag}\"\n",
    "print(image_name)\n",
    "print(image_uri)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60dba6",
   "metadata": {},
   "source": [
    "## 2) Visualize and upload the dataset\n",
    "Take note of the S3 URI here if you get interrupted, no need to reupload later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "855c1822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>service</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/ec2/autoscaling/faqs/</td>\n",
       "      <td>What is Amazon EC2 Auto Scaling?</td>\n",
       "      <td>Amazon EC2 Auto Scaling is a fully managed ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/ec2/autoscaling/faqs/</td>\n",
       "      <td>When should I use Amazon EC2 Auto Scaling vs. ...</td>\n",
       "      <td>You should use AWS Auto Scaling to manage scal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/ec2/autoscaling/faqs/</td>\n",
       "      <td>How is Predictive Scaling Policy different fro...</td>\n",
       "      <td>Predictive Scaling Policy brings the similar p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/ec2/autoscaling/faqs/</td>\n",
       "      <td>What are the benefits of using Amazon EC2 Auto...</td>\n",
       "      <td>Amazon EC2 Auto Scaling helps to maintain your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/ec2/autoscaling/faqs/</td>\n",
       "      <td>What is fleet management and how is it differe...</td>\n",
       "      <td>If your application runs on Amazon EC2 instanc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  service                                           question  \\\n",
       "0  /ec2/autoscaling/faqs/                   What is Amazon EC2 Auto Scaling?   \n",
       "1  /ec2/autoscaling/faqs/  When should I use Amazon EC2 Auto Scaling vs. ...   \n",
       "2  /ec2/autoscaling/faqs/  How is Predictive Scaling Policy different fro...   \n",
       "3  /ec2/autoscaling/faqs/  What are the benefits of using Amazon EC2 Auto...   \n",
       "4  /ec2/autoscaling/faqs/  What is fleet management and how is it differe...   \n",
       "\n",
       "                                             answers  \n",
       "0  Amazon EC2 Auto Scaling is a fully managed ser...  \n",
       "1  You should use AWS Auto Scaling to manage scal...  \n",
       "2  Predictive Scaling Policy brings the similar p...  \n",
       "3  Amazon EC2 Auto Scaling helps to maintain your...  \n",
       "4  If your application runs on Amazon EC2 instanc...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv.gz', compression='gzip', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e1b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uri = sess.upload_data(path='train.csv.gz', key_prefix='datasets/aws-faq/train')\n",
    "print(s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44643986",
   "metadata": {},
   "source": [
    "## 3) Build a custom container image with NeuronSDK 2.9+\n",
    "NeuronSDK 2.9+ is required to deal with T5. We'll take a pre-existing container with NeuronSDK 2.8 and upgrade it. To build the docker image and upload it to ECR, we'll make use of **sagemaker-studio-image-build**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa193542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isdir('container'): os.mkdir('container')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac63cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile container/Dockerfile\n",
    "ARG REGION=us-east-1\n",
    "FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/pytorch-training-neuron:1.13.0-neuron-py38-sdk2.8.0-ubuntu20.04\n",
    "\n",
    "RUN apt update && apt install -y \\\n",
    "    aws-neuronx-dkms=2.* \\\n",
    "    aws-neuronx-tools=2.* \\\n",
    "    aws-neuronx-collectives=2.* \\\n",
    "    aws-neuronx-runtime-lib=2.* \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN pip3 install -U pip && \\\n",
    "    pip3 install --force-reinstall neuronx-cc==2.* torch-neuronx torchvision==0.14.1 transformers==4.27.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af7f09-cb0e-4075-9106-fc97ccc0e866",
   "metadata": {},
   "source": [
    "### Use sm-docker to build our image\n",
    "Due to instance memeory issues, BUILD_GENERAL1_MEDIUM seems to be best.\n",
    "We'll use `image_name` for our repo (will create properly as long as permissions were setup according to the [documentation here](https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de023dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sm-docker build container --repository repo_name --compute-type BUILD_GENERAL1_MEDIUM --build-arg REGION=$region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd77a3",
   "metadata": {},
   "source": [
    "## 4) Prepare the train/inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b28590cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isdir('src'): os.mkdir('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27761ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize src/question_answering.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8b7c0",
   "metadata": {},
   "source": [
    "## 5) Kick-off our fine tuning job on Amazon SageMaker\n",
    "We need to create a SageMaker Estimator first and then invoke **.fit**. \n",
    "\n",
    "Please, notice we're passing the parameter **checkpoint_s3_uri**. This is important because NeuronSDK will spend some time compiling the model before fine tuning it. The compiler saves the model to cache files and, with this param, the files will be uploaded to **S3**. So, next time we run a job, NeuronSDK can just load back the cache files and start training immediately.\n",
    "\n",
    "When training for the first time, the training job takes ~9 hours to process all 60 Epochs on an **trn1.32xlarge**.\n",
    "\n",
    "\n",
    "If you need to wait for a quota increase like I did. When you come back, run cell 2 to setup the sagemaker session and S3 uris, etc. Then run the below to get the process started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually set S3 bucket:\n",
    "# s3_uri = \"s3://sagemaker-<region>-<accountID>/datasets/aws-faq/train/train.csv.gz\"\n",
    "print(s3_uri)\n",
    "\n",
    "# manually set to the image uri given by sm-docker\n",
    "# image_uri = \"<accountID>.dkr.ecr.<region>.amazonaws.com/<repo>:<tag>\"\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ab4e4ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/aws/deep-learning-containers/blob/master/available_images.md#neuron-containers\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"question_answering.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.32xlarge',\n",
    "    image_uri=image_uri,\n",
    "    disable_profiler=True,\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    \n",
    "    # Parameters required to enable checkpointing\n",
    "    # This is necessary for caching XLA HLO files and reduce training time next time    \n",
    "    checkpoint_s3_uri=f\"s3://{bucket}/checkpoints\",\n",
    "    volume_size = 512,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    },\n",
    "    hyperparameters={\n",
    "        \"model-name\": \"t5-small-ssm\",\n",
    "        \"lr\": 5e-5,\n",
    "        \"num-epochs\": 60\n",
    "    },\n",
    "    metric_definitions=[\n",
    "        {'Name': 'train:loss', 'Regex': 'loss:(\\S+);'}\n",
    "    ]\n",
    ")\n",
    "estimator.framework_version = '1.13.1' # workround when using image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4e4ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit({\"train\": s3_uri})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f082b4",
   "metadata": {},
   "source": [
    "## 6) Deploy our model to a SageMaker endpoint\n",
    "Here, we're using a pre-defined HuggingFace model class+container to just load our fine tuned model on a CPU based instance: c6i.4xlarge (an Intel Xeon based machine).\n",
    "\n",
    ">If you're picking this up later uncomment line 4, fill in the path to your model artifacts, comment line 9 out, and uncomment line 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d90af272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and modify this if you're picking this back up later and your training was sucessful.\n",
    "# you'll need to get the model s3 URI from sagemaker -> Training -> Training Jobs -> <Your job name> -> Output -> S3 model artifact\n",
    "\n",
    "# pre_trained_model = YOUR_S3_PATH\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=estimator.model_data,       # path to your model and script\n",
    "   # model_data=pre_trained_model,       # path to your model and script\n",
    "   role=role,                             # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.26.0\",         # transformers version used\n",
    "   pytorch_version=\"1.13.1\",              # pytorch version used\n",
    "   py_version='py39',                     # python version used\n",
    "   sagemaker_session=sess,\n",
    "   \n",
    "   # for production it is important to define vpc_config and use a vpc_endpoint\n",
    "   #vpc_config={\n",
    "   #    'Subnets': ['subnet-A-REPLACE', 'subnet-B-REPLACE'],\n",
    "   #    'SecurityGroupIds': ['sg-A-REPLACE', 'sg-B-REPLACE']\n",
    "   #}    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.c6i.4xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f12446",
   "metadata": {},
   "source": [
    "## 7) Run a quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e393b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is SageMaker?\n",
      "A: SageMaker is a new ML (ML) service that makes it easy to build, train, and deploy notebook data inference, and deploy and tune models of data. SageMaker helps you build, train, and manage your ML models, and deploy model data to build your models up and down\n",
      "\n",
      "Q: What is EC2 AutoScaling?\n",
      "A: Amazon-based EC2 instancess let you reduce your applications on multiple factors by allowing you to scale your application requirements and costs across multiple instances. Amazoning EC2 instances as a result of optimization in your applications, reducing the number of compute EC and the number of available instances to optimize your\n",
      "\n",
      "Q: What are the benefits of autoscaling?\n",
      "A: You can use autoscaling to help you optimize the capacity of your applications by allowing you to take advantage of your application across multiple applications. Autoscaling allows you to easily scale the number of your applications across multiple devices, and optimize your fleet up or down to 40%. You can also use auto\n",
      "\n",
      "CPU times: user 5.16 ms, sys: 0 ns, total: 5.16 ms\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "questions = [\n",
    "    \"What is SageMaker?\",\n",
    "    \"What is EC2 AutoScaling?\",\n",
    "    \"What are the benefits of autoscaling?\"\n",
    "]\n",
    "resp = predictor.predict({'inputs': questions})\n",
    "for q,a in zip(questions, resp['answers']):\n",
    "    print(f\"Q: {q}\\nA: {a}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd7c8a",
   "metadata": {},
   "source": [
    "## 8) Clean up\n",
    "This will delete the model and the endpoint you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
