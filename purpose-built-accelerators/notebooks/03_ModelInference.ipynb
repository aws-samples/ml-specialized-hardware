{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59eae6c3-87ba-442b-a93c-0e6b9d6b1cb8",
   "metadata": {},
   "source": [
    "# AWS Machine Learning Purpose-built Accelerators Tutorial\n",
    "## Learn how to use [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/) with [Amazon SageMaker](https://aws.amazon.com/sagemaker/), to optimize your ML workload\n",
    "## Part 3/3 - Compiling and deploying a Bert model to AWS Inferentia 2 with SageMaker + [Hugging Face Optimum Neuron](https://huggingface.co/docs/optimum-neuron/index)\n",
    "\n",
    "**SageMaker studio Kernel: PyTorch 1.13 Python 3.9 CPU - ml.t3.medium** \n",
    "\n",
    "In this tutorial, you'll learn how to compile a model to AWS Inferentia and then deploy it to a SageMaker real-time endpoint powered by AWS Inferentia2. First we'll kick-off a SageMaker job to compile the model. We need to do this once. After that, we can deploy our model to a SageMaker endpoint and finally get some predictions.\n",
    "\n",
    "In section 02, you extract some metadata from the Optimum Neuron API and render a table with the current tested/supported models (similar models not listed there can also be compatible, but you need to check by yourself). This table is important for you to understand which models can be selected for deployment. However, if you also need to fine-tune your model, check a similar table in the notebook **Part 2** to see which models can be fine-tuned with AWS Trainium using HF Optimum Neuron. That way you can plan your end2end solution and start implementing it right now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06990e00-6436-429a-a835-47fb4b2012f5",
   "metadata": {},
   "source": [
    "## 1) Install some required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560e1d5-cdc9-41be-8f39-a1b811b161c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U optimum-neuron==0.0.8 onnx>=1.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d435090-f88c-4241-bfa2-1a5bf6c0e39b",
   "metadata": {},
   "source": [
    "## 2) Supported models/tasks for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf36f85b-3416-4be5-b092-fdccfb496f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, Markdown\n",
    "from optimum.exporters.tasks import TasksManager\n",
    "from optimum.exporters.neuron.model_configs import *\n",
    "from optimum.neuron.distributed.parallelizers_manager import ParallelizersManager\n",
    "from optimum.neuron.utils.training_utils import (\n",
    "    _SUPPORTED_MODEL_NAMES,\n",
    "    _SUPPORTED_MODEL_TYPES,\n",
    "    _generate_supported_model_class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5880a8-762e-4fcb-95dd-c6ef120be393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve supported models for Tensor Parallelism\n",
    "tp_support = list(ParallelizersManager._MODEL_TYPE_TO_PARALLEL_MODEL_CLASS.keys())\n",
    "\n",
    "# build compability table for inference\n",
    "meta = [(k,list(v['neuron'].keys())) for k,v in TasksManager._SUPPORTED_MODEL_TYPE.items() if v.get('neuron') is not None]\n",
    "data_inference = {'Model': []}\n",
    "for m,t in meta:\n",
    "    model_id = len(data_inference['Model'])\n",
    "    model_link = f'<a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search={m}\">{m}</a>'\n",
    "    data_inference['Model'].append(f\"{model_link} <font style='color: red;'><b>[TP]</b></font>\" if m in tp_support else model_link)\n",
    "    for task in t:\n",
    "        if data_inference.get(task) is None: data_inference[task] = [''] * len(meta)\n",
    "        data_inference[task][model_id] = f'<a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag={task}&sort=trending&search={m}\">list</a>'\n",
    "\n",
    "df_inference = pd.DataFrame.from_dict(data_inference).set_index('Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df935ba7-5d9f-4eed-a641-84dd2de7e7a9",
   "metadata": {},
   "source": [
    "In each new release of HF Optimum Neuron, support for new models is added. So, it is expected to see different values for the following tables when you upgrade the library.\n",
    "\n",
    "Models with **[TP]** after the name support Tensor Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "506c74d9-6a61-482f-a3c1-bcf796efb1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model                                                                                                                                       | feature-extraction                                                                                                                  | fill-mask                                                                                                              | multiple-choice                                                                                                              | question-answering                                                                                                              | text-classification                                                                                                              | token-classification                                                                                                              | zero-shot-image-classification                                                                                                       | stable-diffusion                                                                                                                  | text-generation                                                                                                       | semantic-segmentation                                                                                                              |\n",
       "|:--------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------|\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=albert\">albert</a>                                                | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=albert\">list</a>          | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=albert\">list</a>      | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=albert\">list</a>      | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=albert\">list</a>      | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=albert\">list</a>      | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=albert\">list</a>      |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=bert\">bert</a> <font style='color: red;'><b>[TP]</b></font>       | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=bert\">list</a>            | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=bert\">list</a>        | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=bert\">list</a>        | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=bert\">list</a>        | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=bert\">list</a>        | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=bert\">list</a>        |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=camembert\">camembert</a>                                          | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=camembert\">list</a>       | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=camembert\">list</a>   | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=camembert\">list</a>   | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=camembert\">list</a>   | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=camembert\">list</a>   | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=camembert\">list</a>   |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=clip\">clip</a>                                                    | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=clip\">list</a>            |                                                                                                                        |                                                                                                                              |                                                                                                                                 |                                                                                                                                  |                                                                                                                                   | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&sort=trending&search=clip\">list</a> |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=clip-text-model\">clip-text-model</a>                              | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=clip-text-model\">list</a> |                                                                                                                        |                                                                                                                              |                                                                                                                                 |                                                                                                                                  |                                                                                                                                   |                                                                                                                                      | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=stable-diffusion&sort=trending&search=clip-text-model\">list</a> |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=convbert\">convbert</a>                                            | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=convbert\">list</a>        | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=convbert\">list</a>    | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=convbert\">list</a>    | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=convbert\">list</a>    | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=convbert\">list</a>    | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=convbert\">list</a>    |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=deberta\">deberta</a>                                              | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=deberta\">list</a>         | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=deberta\">list</a>     | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=deberta\">list</a>     | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=deberta\">list</a>     | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=deberta\">list</a>     | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=deberta\">list</a>     |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=deberta-v2\">deberta-v2</a>                                        | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=deberta-v2\">list</a>      | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=deberta-v2\">list</a>  | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=deberta-v2\">list</a>  | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=deberta-v2\">list</a>  | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=deberta-v2\">list</a>  | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=deberta-v2\">list</a>  |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=distilbert\">distilbert</a>                                        | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=distilbert\">list</a>      | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=distilbert\">list</a>  | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=distilbert\">list</a>  | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=distilbert\">list</a>  | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=distilbert\">list</a>  | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=distilbert\">list</a>  |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=electra\">electra</a>                                              | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=electra\">list</a>         | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=electra\">list</a>     | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=electra\">list</a>     | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=electra\">list</a>     | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=electra\">list</a>     | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=electra\">list</a>     |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=flaubert\">flaubert</a>                                            | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=flaubert\">list</a>        | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=flaubert\">list</a>    | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=flaubert\">list</a>    | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=flaubert\">list</a>    | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=flaubert\">list</a>    | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=flaubert\">list</a>    |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=gpt2\">gpt2</a>                                                    |                                                                                                                                     |                                                                                                                        |                                                                                                                              |                                                                                                                                 |                                                                                                                                  |                                                                                                                                   |                                                                                                                                      |                                                                                                                                   | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-generation&sort=trending&search=gpt2\">list</a> |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=mobilebert\">mobilebert</a>                                        | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=mobilebert\">list</a>      | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=mobilebert\">list</a>  | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=mobilebert\">list</a>  | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=mobilebert\">list</a>  | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=mobilebert\">list</a>  | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=mobilebert\">list</a>  |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=mpnet\">mpnet</a>                                                  | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=mpnet\">list</a>           | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=mpnet\">list</a>       | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=mpnet\">list</a>       | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=mpnet\">list</a>       | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=mpnet\">list</a>       | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=mpnet\">list</a>       |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=roberta\">roberta</a> <font style='color: red;'><b>[TP]</b></font> | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=roberta\">list</a>         | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=roberta\">list</a>     | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=roberta\">list</a>     | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=roberta\">list</a>     | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=roberta\">list</a>     | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=roberta\">list</a>     |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=roformer\">roformer</a>                                            | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=roformer\">list</a>        | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=roformer\">list</a>    | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=roformer\">list</a>    | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=roformer\">list</a>    | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=roformer\">list</a>    | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=roformer\">list</a>    |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=unet\">unet</a>                                                    |                                                                                                                                     |                                                                                                                        |                                                                                                                              |                                                                                                                                 |                                                                                                                                  |                                                                                                                                   |                                                                                                                                      | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=stable-diffusion&sort=trending&search=unet\">list</a>            |                                                                                                                       | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=semantic-segmentation&sort=trending&search=unet\">list</a>        |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=vae-encoder\">vae-encoder</a>                                      |                                                                                                                                     |                                                                                                                        |                                                                                                                              |                                                                                                                                 |                                                                                                                                  |                                                                                                                                   |                                                                                                                                      | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=stable-diffusion&sort=trending&search=vae-encoder\">list</a>     |                                                                                                                       | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=semantic-segmentation&sort=trending&search=vae-encoder\">list</a> |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=vae-decoder\">vae-decoder</a>                                      |                                                                                                                                     |                                                                                                                        |                                                                                                                              |                                                                                                                                 |                                                                                                                                  |                                                                                                                                   |                                                                                                                                      | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=stable-diffusion&sort=trending&search=vae-decoder\">list</a>     |                                                                                                                       | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=semantic-segmentation&sort=trending&search=vae-decoder\">list</a> |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=xlm\">xlm</a>                                                      | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=xlm\">list</a>             | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=xlm\">list</a>         | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=xlm\">list</a>         | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=xlm\">list</a>         | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=xlm\">list</a>         | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=xlm\">list</a>         |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |\n",
       "| <a target=\"_new\" href=\"https://huggingface.co/models?sort=trending&search=xlm-roberta\">xlm-roberta</a>                                      | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=feature-extraction&sort=trending&search=xlm-roberta\">list</a>     | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=fill-mask&sort=trending&search=xlm-roberta\">list</a> | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=multiple-choice&sort=trending&search=xlm-roberta\">list</a> | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=question-answering&sort=trending&search=xlm-roberta\">list</a> | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=xlm-roberta\">list</a> | <a target=\"_new\" href=\"https://huggingface.co/models?pipeline_tag=token-classification&sort=trending&search=xlm-roberta\">list</a> |                                                                                                                                      |                                                                                                                                   |                                                                                                                       |                                                                                                                                    |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(df_inference.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2245d70a-bfa2-40a7-82a3-b61bc3577e92",
   "metadata": {},
   "source": [
    "## 3) Compiling a pre-trained model to AWS Inferentia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216e034-62a5-40cf-bc00-c63eabe3f0e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.146.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "if not os.path.isdir('src'): os.makedirs('src', exist_ok=True)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb57433-7214-4203-a56c-38648187e3b7",
   "metadata": {},
   "source": [
    "### 3.1) Re-pack the checkpoints from the fine tuning job (previous notebook)\n",
    "\n",
    "In this step, you'll download the checkpoints create in the fine tuning job and re-pack them for compiling them to Inferentia2.\n",
    "\n",
    "Copy the SageMaker training job name from the previous notebook **02_ModelFineTuning** or from your AWS Console/SageMaker and set the variable **training_job_name**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f86f64-3901-4023-add3-00a55da1ef99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import tarfile\n",
    "\n",
    "training_job_name=\"\"\n",
    "if os.path.isfile(\"training_job_name.txt\"): training_job_name = open(\"training_job_name.txt\", \"r\").read()\n",
    "assert len(training_job_name)>0, \"Please copy the name of the training_job you ran in the previous notebook and set training_job_name\"\n",
    "\n",
    "# Extract the artifacts\n",
    "if not os.path.isdir(\"checkpoints\"):\n",
    "    downloader = sagemaker.s3.S3Downloader()\n",
    "    print(\"Download checkpoints from S3...\")\n",
    "    data = downloader.read_bytes(f\"s3://{bucket}/output/{training_job_name}/output/model.tar.gz\")\n",
    "    print(\"Extracting package...\")\n",
    "    with tarfile.open(fileobj=io.BytesIO(data), mode='r:gz') as tar:\n",
    "        tar.extractall(\"checkpoints\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e0c21-d542-4225-b825-0fa479d8af55",
   "metadata": {},
   "source": [
    "#### 3.1.1) Upload the new re-packed checkpoint to SageMaker\n",
    "These artifacts will be used by our compilation script (defined below) to compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26221a49-e070-44d2-ad69-c2b1760112b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "checkpoints = glob.glob(\"checkpoints/checkpoint-*\")\n",
    "assert len(checkpoints)>0, \"No checkpoint found in the directory model\"\n",
    "print(f\"Uploading checkpoint: {checkpoints[0]} ...\")\n",
    "s3_checkpoint_uri = sess.upload_data(checkpoints[0], bucket=bucket, key_prefix=\"models/spam-classifier\")\n",
    "print(f\"S3 URI: {s3_checkpoint_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a27737-3dbd-46f0-b098-e6ff72627b49",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2) Compiling script that will be invoked by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849bb42-1a85-489a-b621-615f9f1d917b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/compile.py\n",
    "import os\n",
    "os.environ['NEURON_RT_NUM_CORES'] = '1'\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "import logging\n",
    "import argparse\n",
    "import traceback\n",
    "import optimum.neuron\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TASK=\"<<TASK>>\"\n",
    "\n",
    "def model_fn(model_dir, context=None):\n",
    "    global TASK\n",
    "    if \"TASK\" in TASK: raise Exception(\"Invalid TASK. You need to invoke the compilation job once to set TASK variable\")\n",
    "        \n",
    "    NeuronModel = eval(f\"optimum.neuron.NeuronModelFor{TASK}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    \n",
    "    model = NeuronModel.from_pretrained(model_dir)\n",
    "    return model,tokenizer\n",
    "\n",
    "def input_fn(input_data, content_type, context=None):\n",
    "    if content_type == 'application/json':\n",
    "        req = json.loads(input_data)\n",
    "        prompt = req.get('prompt')\n",
    "        if prompt is None or len(prompt) < 3:\n",
    "            raise(\"Invalid prompt. Provide an input like: {'prompt': 'text text text'}\")\n",
    "        return prompt\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported mime type: {content_type}. Supported: application/json\")    \n",
    "\n",
    "def predict_fn(input_object, model_tokenizer, context=None):\n",
    "    model,tokenizer = model_tokenizer\n",
    "    inputs = tokenizer(input_object, truncation=True, return_tensors=\"pt\")\n",
    "    logits = model(**inputs).logits\n",
    "    idx = logits.argmax(1, keepdim=True)\n",
    "    conf = torch.gather(logits, 1, idx)\n",
    "    return torch.cat([idx,conf], 1)    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.    \n",
    "    parser.add_argument(\"--task\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--input_shapes\", type=str, required=True)\n",
    "    \n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])    \n",
    "    parser.add_argument(\"--checkpoint_dir\", type=str, default=os.environ[\"SM_CHANNEL_CHECKPOINT\"])\n",
    "    \n",
    "    try:\n",
    "        args, _ = parser.parse_known_args()\n",
    "        \n",
    "        # Set up logging        \n",
    "        logging.basicConfig(\n",
    "            level=logging.getLevelName(\"INFO\"),\n",
    "            handlers=[logging.StreamHandler(sys.stdout)],\n",
    "            format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "        )\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.info(args)\n",
    "\n",
    "        NeuronModel = eval(f\"optimum.neuron.NeuronModel{'For' + args.task if len(args.task) > 0 else ''}\")\n",
    "        logger.info(f\"Checkpoint files: {os.listdir(args.checkpoint_dir)}\")\n",
    "        \n",
    "        input_shapes = json.loads(args.input_shapes)\n",
    "        model = NeuronModel.from_pretrained(args.checkpoint_dir, export=True, **input_shapes)\n",
    "        model.save_pretrained(args.model_dir)\n",
    "\n",
    "        code_path = os.path.join(args.model_dir, 'code')\n",
    "        os.makedirs(code_path, exist_ok=True)\n",
    "        \n",
    "        with open(__file__, 'r') as f:\n",
    "            content = f.read()\n",
    "            content = content.replace(\"<<TASK>>\", \"SequenceClassification\")\n",
    "            with open(os.path.join(code_path, \"inference.py\"), \"w\") as i:\n",
    "                i.write(content)\n",
    "        shutil.copyfile('requirements.txt', os.path.join(code_path, 'requirements.txt'))\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        sys.exit(1)\n",
    "        \n",
    "    finally:\n",
    "        print(\"Done! \", sys.exc_info())\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a6f646-8306-4572-b4d1-496e8012bd20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "neuronx-distributed\n",
    "## 4.30 or higher is required\n",
    "transformers==4.30.0\n",
    "optimum-neuron==0.0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f815449-7358-414a-ba98-bfa8afc69432",
   "metadata": {},
   "source": [
    "### 3.3) SageMaker Estimator\n",
    "This object will help you to configure the compilation job (SageMaker Training Job).\n",
    "\n",
    "This job will invoke **compile.py** script, which will compile our model to Inferentia2 and than save the artifacts for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c6bd9-aa4a-44b9-86ee-d4a646d17453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "input_shapes={\"batch_size\": 1, \"sequence_length\": 512}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"compile.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.2xlarge',\n",
    "    output_path=f\"s3://{bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    \n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.12.0-ubuntu20.04\",\n",
    "    \n",
    "    volume_size = 512,\n",
    "    hyperparameters={     \n",
    "        \"task\": \"SequenceClassification\",\n",
    "        \"input_shapes\": f\"'{json.dumps(input_shapes)}'\",\n",
    "    }\n",
    ")\n",
    "estimator.framework_version = '1.13.1' # workround when using image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbad0c1-adc0-44e9-8985-27c797924610",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({\"checkpoint\": s3_checkpoint_uri})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f88afd2-8eb8-4a95-bfdf-2ec416432657",
   "metadata": {},
   "source": [
    "## 4) Deploy a SageMaker real-time endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13bd976-533e-4172-908e-a869c9ccaf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# depending on the inf2 instance you deploy the model you'll have more or less accelerators\n",
    "# we'll ask SageMaker to launch 1 worker per core\n",
    "\n",
    "model_data=estimator.model_data\n",
    "print(f\"Model data: {model_data}\")\n",
    "\n",
    "instance_type_idx=0 # default ml.inf2.xlarge\n",
    "instance_types=['ml.inf2.xlarge', 'ml.inf2.8xlarge', 'ml.inf2.24xlarge','ml.inf2.48xlarge']\n",
    "num_workers=[2,2,12,24]\n",
    "\n",
    "print(f\"Instance type: {instance_types[instance_type_idx]}. Num SM workers: {num_workers[instance_type_idx]}\")\n",
    "pytorch_model = PyTorchModel(\n",
    "    image_uri=f\"763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.12.0-ubuntu20.04\",\n",
    "    model_data=model_data,\n",
    "    role=role,    \n",
    "    name=name_from_base('bert-spam-classifier'),\n",
    "    sagemaker_session=sess,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    model_server_workers=num_workers[instance_type_idx], # 1 worker per inferentia chip\n",
    "    framework_version=\"1.13.1\",\n",
    "    env = {\n",
    "        'SAGEMAKER_MODEL_SERVER_TIMEOUT' : '3600' \n",
    "    }\n",
    "    # for production it is important to define vpc_config and use a vpc_endpoint\n",
    "    #vpc_config={\n",
    "    #    'Subnets': ['<SUBNET1>', '<SUBNET2>'],\n",
    "    #    'SecurityGroupIds': ['<SECURITYGROUP1>', '<DEFAULTSECURITYGROUP>']\n",
    "    #}\n",
    ")\n",
    "pytorch_model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4936386-2f04-4325-b793-e21fbe8997f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_types[instance_type_idx],\n",
    "    model_data_download_timeout=3600, # it takes some time to download all the artifacts and load the model\n",
    "    container_startup_health_check_timeout=1800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b8086-a665-4ca8-92c3-8b6c0ebef7d1",
   "metadata": {},
   "source": [
    "## 5) Run a simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c41b2-4440-4e5a-9349-0ccda8f42e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c20beb52-ef40-4beb-a15e-54cf591cef9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 43.573856353759766\n",
      "Pred: 0 - not spam / score: 4.785090923309326\n",
      "Elapsed time: 33.95414352416992\n",
      "Pred: 1 - spam / score: 4.689364910125732\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "labels={0: \"not spam\", 1: \"spam\"}\n",
    "not_spam=\" Deezer.com 10,406,168 Artist DB\\n\\nWe have scraped the Deezer Artist DB, right now there are 10,406,168 listings according to Deezer.com\\n\\nPlease note in going through part of the list, it is obvious there are mistakes inside their system.\\n\\nExamples include and Artist with &amp; in its name might also be found with \"and\" but the Albums for each have different totals etc. Have no clue if there are duplicate albums etc do this error in their system. Even a comma in a name could mean the Artist shows up more than once, I saw in 1 instance that 1 Artist had 6 different ArtistIDs due to spelling errors.\\n\\nSo what is this DB, very simple, it gives you the ArtistID and the actual name of the Artist in another column. If you want to see the artist you add the baseurl to the ArtistID\\n\\nAn example is ArtistID 115 is AC/DC\\n\\n[https://www.deezer.com/us/artist/115](https://www.deezer.com/us/artist/115)\\n\\nYou do not have to use [https://www.deezer.com/us/artist/](https://www.deezer.com/us/artist/) if your first language is other than English, just see if Deezer supports your language and use that baseref\\n\\nFrench for example is [https://www.deezer.com/fr/artist/115](https://www.deezer.com/fr/artist/115)\\n\\nI am providing the DB in 3 different formats:\\n\\n \\n\\nI tried posting download links here but it seems Reddit does not like that so get them here:\\n\\n[https://pastebin\\\\[DOT\\\\]com/V3KJbgif](https://pastebin.com/V3KJbgif)\\n\\n&amp;#x200B;\\n\\n**Special thanks go to** [**/user/KoalaBear84**](https://www.reddit.com/user/KoalaBear84) **for writing the scraper.**\\n\\n&amp;#x200B;\\n\\n**Cross Posted to related Reddit Groups**\"\n",
    "spam=\" ATTENTION ALL USERS! \\n\\n Are you looking for a way to GET RICH QUICK? \\n\\n Don't waste your time with boring old jobs! \\n\\n Join our CRAZY MONEY-MAKING SYSTEM today! \\n\\n Just sign up and start earning BIG BUCKS right away! \\n\\n Plus, if you refer your friends, you'll get even MORE CASH! \\n\\n This is the HOTTEST OFFER of the year! \\n\\n Don't wait\"\n",
    "\n",
    "for i,text in enumerate([not_spam, spam]):\n",
    "    t=time.time()\n",
    "    pred = predictor.predict({\"prompt\": text})\n",
    "    elapsed = (time.time()-t)*1000\n",
    "    print(f\"Elapsed time: {elapsed}\")\n",
    "    print(f\"Pred: {i} - {labels[pred[0][0]]} / score: {pred[0][1]}\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
