{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4c51b2-2b7c-480c-bf73-e918d0fe2edb",
   "metadata": {},
   "source": [
    "# Deploy Whisper model on AWS Inference 2 with manual porting\n",
    "Manual porting is another option when using AWS Neuron Chips. This typically requires understanding well the model architecture so it can be split and its individual components compiled.\n",
    "\n",
    "First install dependencies and download test file. You can skip this step if you executed [01_Whisper_gpu](01_Whisper_gpu.ipynb) notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c207e-3c22-44cc-a80d-abd16ff526e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U transformers==4.36.2 datasets==2.18.0 soundfile==0.12.1 librosa==0.10.1 sagemaker\n",
    "!wget --no-check-certificate https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db89cf7-8a98-47d2-8211-5b4f2ede68b8",
   "metadata": {},
   "source": [
    "Remember to restart kernel after installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da4d16-57a2-4c96-a67d-56709db7b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "inf_region = 'us-east-2'\n",
    "\n",
    "session = sagemaker.Session(boto_session=boto3.Session(region_name=inf_region))\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role(sagemaker_session=session)\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "print(role)\n",
    "\n",
    "inf_bucket = session.default_bucket()\n",
    "print(inf_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74fe2d-d2bf-4234-afac-1f573891ddc3",
   "metadata": {},
   "source": [
    "## Compile model to neuron\n",
    "Before we deploy the model we need to compile it so it can run on neuron devices. To do that we will use training job on Amazon Sagemaker that will run the compilation script and export compiled model to s3.\n",
    "\n",
    "Unlike with the Optimum Neuron example [02_Whisper_optimum_neuron](02_Whisper_optimum_neuron.ipynb), here we have to be specific with regards to how we want to compile the model, instead of relying on ON to do it for us.\n",
    "\n",
    "Let's start with creating `src` directory when we put requirements.txt file for the compilation job and compilation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2f627-28ac-4a9c-8916-4e2c5a033125",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4c183-7c45-4365-a47e-c25a04b2eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "--extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "transformers==4.36.2\n",
    "datasets==2.18.0 \n",
    "soundfile==0.12.1 \n",
    "librosa==0.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d906e-9af8-447b-8dfd-39ba84929b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/compile.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import argparse\n",
    "import torch\n",
    "import types\n",
    "import torch_neuronx\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions,BaseModelOutput\n",
    "\n",
    "os.environ['NEURON_RT_NUM_CORES']='1'\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_model(model_id):\n",
    "    logger.info(f\"Loading model: {model_id}\")\n",
    "    processor = WhisperProcessor.from_pretrained(model_id)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_id, torchscript=True)\n",
    "    logger.info(f\"Model loaded - encoder dim: {model.config.num_mel_bins}, decoder dim: {model.config.d_model}\")\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def enc_f(self, input_features, attention_mask, **kwargs):\n",
    "    if hasattr(self, 'forward_neuron'):\n",
    "        out = self.forward_neuron(input_features, attention_mask)\n",
    "    else:\n",
    "        out = self.forward_(input_features, attention_mask, return_dict=True)\n",
    "    return BaseModelOutput(**out)\n",
    "\n",
    "\n",
    "def dec_f(self, input_ids, attention_mask=None, encoder_hidden_states=None, **kwargs):\n",
    "    out = None\n",
    "    if not attention_mask is None and encoder_hidden_states is None:\n",
    "        encoder_hidden_states, attention_mask = attention_mask,encoder_hidden_states\n",
    "    inp = [input_ids, encoder_hidden_states]\n",
    "\n",
    "    if inp[0].shape[1] > self.max_length:\n",
    "        logger.error(f\"Decoded sequence length {inp[0].shape[1]} exceeds max {self.max_length}\")\n",
    "        raise Exception(f\"The decoded sequence is not supported. Max: {self.max_length}\")\n",
    "    pad_size = torch.as_tensor(self.max_length - inp[0].shape[1])\n",
    "    inp[0] = F.pad(inp[0], (0, pad_size), \"constant\", processor.tokenizer.pad_token_id)\n",
    "\n",
    "    if hasattr(self, 'forward_neuron'):\n",
    "        out = self.forward_neuron(*inp)\n",
    "    else:\n",
    "        out = self.forward_(input_ids=inp[0], encoder_hidden_states=inp[1], return_dict=True, use_cache=False, output_attentions=output_attentions)\n",
    "    out['last_hidden_state'] = out['last_hidden_state'][:, :input_ids.shape[1], :]\n",
    "    if not out.get('attentions') is None:\n",
    "        out['attentions'] = torch.stack([torch.mean(o[:, :, :input_ids.shape[1], :input_ids.shape[1]], axis=2, keepdim=True) for o in out['attentions']])\n",
    "    if not out.get('cross_attentions') is None:\n",
    "        out['cross_attentions'] = torch.stack([torch.mean(o[:, :, :input_ids.shape[1], :], axis=2, keepdim=True) for o in out['cross_attentions']])\n",
    "    return BaseModelOutputWithPastAndCrossAttentions(**out)\n",
    "\n",
    "\n",
    "def proj_out_f(self, inp):\n",
    "    pad_size = torch.as_tensor(self.max_length - inp.shape[1], device=inp.device)\n",
    "    if inp.shape[1] > self.max_length:\n",
    "        logger.error(f\"Input sequence length {inp.shape[1]} exceeds max {self.max_length}\")\n",
    "        raise Exception(f\"The decoded sequence is not supported. Max: {self.max_length}\")\n",
    "    x = F.pad(inp, (0,0,0,pad_size), \"constant\", processor.tokenizer.pad_token_id)\n",
    "\n",
    "    if hasattr(self, 'forward_neuron'):\n",
    "        out = self.forward_neuron(x)\n",
    "    else:\n",
    "        out = self.forward_(x)\n",
    "    out = out[:, :inp.shape[1], :]\n",
    "    return out\n",
    "\n",
    "\n",
    "def update_forward_methods(model, max_dec_len=35):\n",
    "    logger.info(\"Updating forward methods for encoder, decoder, and projection output\")\n",
    "    if not hasattr(model.model.encoder, 'forward_'): model.model.encoder.forward_ = model.model.encoder.forward\n",
    "    if not hasattr(model.model.decoder, 'forward_'): model.model.decoder.forward_ = model.model.decoder.forward\n",
    "    if not hasattr(model.proj_out, 'forward_'): model.proj_out.forward_ = model.proj_out.forward\n",
    "\n",
    "    model.model.encoder.forward = types.MethodType(enc_f, model.model.encoder)\n",
    "    model.model.decoder.forward = types.MethodType(dec_f, model.model.decoder)\n",
    "    model.proj_out.forward = types.MethodType(proj_out_f, model.proj_out)\n",
    "\n",
    "    model.model.decoder.max_length = max_dec_len\n",
    "    model.proj_out.max_length = max_dec_len\n",
    "    logger.info(f\"Forward methods updated - max_length set to {max_dec_len}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def trace_encoder(model, model_dir):\n",
    "    logger.info(\"Starting encoder tracing\")\n",
    "    dim_enc=model.config.num_mel_bins\n",
    "    inp = (torch.zeros([1, dim_enc, 3000], dtype=torch.float32), torch.zeros([1, dim_enc], dtype=torch.int64))\n",
    "    if hasattr(model.model.encoder, 'forward_neuron'): del model.model.encoder.forward_neuron\n",
    "    neuron_encoder = torch_neuronx.trace(\n",
    "        model.model.encoder,\n",
    "        inp,\n",
    "        compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16',\n",
    "        compiler_workdir='./enc_dir',\n",
    "        inline_weights_to_neff=False)\n",
    "    save_path = model_dir+\"/\"+\"enc.neuron\"\n",
    "    neuron_encoder.save(save_path)\n",
    "    logger.info(f\"Encoder traced and saved to {save_path}\")\n",
    "    del inp, neuron_encoder\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def trace_decoder(model, model_dir, max_dec_len):\n",
    "    logger.info(\"Starting decoder tracing\")\n",
    "    dim_dec=model.config.d_model\n",
    "    inp = (torch.zeros([1, max_dec_len], dtype=torch.int64), torch.zeros([1, 1500, dim_dec], dtype=torch.float32))\n",
    "    if hasattr(model.model.decoder, 'forward_neuron'): del model.model.decoder.forward_neuron\n",
    "    neuron_decoder = torch_neuronx.trace(\n",
    "        model.model.decoder,\n",
    "        inp,\n",
    "        compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16',\n",
    "        compiler_workdir='./dec_dir',\n",
    "        inline_weights_to_neff=True)\n",
    "    save_path = model_dir+\"/\"+\"dec.neuron\"\n",
    "    neuron_decoder.save(save_path)\n",
    "    logger.info(f\"Decoder traced and saved to {save_path}\")\n",
    "    del inp, neuron_decoder\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def proj_output(model, model_dir, max_dec_len):\n",
    "    logger.info(\"Starting projection output tracing\")\n",
    "    dim_dec=model.config.d_model\n",
    "    inp = torch.zeros([1, max_dec_len, dim_dec], dtype=torch.float32)\n",
    "    if hasattr(model.proj_out, 'forward_neuron'): del model.proj_out.forward_neuron\n",
    "    neuron_decoder = torch_neuronx.trace(\n",
    "        model.proj_out,\n",
    "        inp,\n",
    "        compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16',\n",
    "        compiler_workdir='./proj_out_dir',\n",
    "        inline_weights_to_neff=True)\n",
    "    save_path = model_dir+\"/\"+\"proj.neuron\"\n",
    "    neuron_decoder.save(save_path)\n",
    "    logger.info(f\"Projection output traced and saved to {save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"Number of samples processed in each batch during training or inference\")\n",
    "    parser.add_argument(\"--max_dec_len\", type=int, default=448, help=\"Maximum sequence length for input data\")\n",
    "    parser.add_argument(\"--hf_token\", type=str, default=None, help=\"Which is used for authentication with Hugging Face's model hub\")\n",
    "    parser.add_argument(\"--model_id\", type=str, default=\"openai/whisper-large-v3\", help=\"Specifies the id for the pre-trained model to be used\")\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--output_attentions\", action=\"store_true\", help=\"Enable output attentions\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    logger.info(f\"Starting model compilation with args: {args}\")\n",
    "\n",
    "    if not args.hf_token is None and len(args.hf_token) > 0:\n",
    "        logger.info(\"HF token provided - logging in to Hugging Face\")\n",
    "        login(token=args.hf_token)\n",
    "\n",
    "    compiler_args = {\"auto_cast\": \"all\", \"auto_cast_type\": \"bf16\", \"model_type\": \"transformer\"}\n",
    "\n",
    "    try:\n",
    "        model, processor = load_model(model_id=args.model_id)\n",
    "\n",
    "        model = update_forward_methods(model,\n",
    "                                       max_dec_len=args.max_dec_len)\n",
    "\n",
    "        global output_attentions\n",
    "        output_attentions = args.output_attentions\n",
    "\n",
    "        trace_encoder(model, args.model_dir)\n",
    "        trace_decoder(model, args.model_dir, args.max_dec_len)\n",
    "        proj_output(model, args.model_dir, args.max_dec_len)\n",
    "\n",
    "        logger.info(\"Model compilation completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model compilation failed: {str(e)}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ecc19-b894-40c4-a1f4-0f36ce7d696d",
   "metadata": {},
   "source": [
    "Define the training job and run it. We are using trn1.2xlarge instance because compilation requires extra amount of memory then running the model. We will use AWS Inference 2 later to deploy already compiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3dcf40-c0d7-4545-aa33-9b8b5ef770e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "HF_TOKEN=\"\"\n",
    "tp_degree=1\n",
    "max_dec_len=35 #--> This was chosen for our audio sample of 13s\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "hyperparameters={\n",
    "    \"max_dec_len\": max_dec_len,\n",
    "    \"batch_size\": 1,\n",
    "    \"model_id\": model_id\n",
    "}\n",
    "\n",
    "if HF_TOKEN and len(HF_TOKEN) > 3:\n",
    "    hyperparameters[\"hf_token\"]= HF_TOKEN\n",
    "    \n",
    "estimator = PyTorch(\n",
    "    entry_point=\"compile.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=session,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.2xlarge',\n",
    "    output_path=f\"s3://{inf_bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    disable_output_compression=True,\n",
    "\n",
    "\n",
    "    image_uri=f\"763104351884.dkr.ecr.{inf_region}.amazonaws.com/pytorch-training-neuronx:2.7.0-neuronx-py310-sdk2.25.0-ubuntu22.04\",\n",
    "    env={\n",
    "        'NEURON_RT_NUM_CORES': str(tp_degree)\n",
    "    },\n",
    "    # volume_size = 512,\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f15c3-5b45-4d85-9cb8-656a64ff42f7",
   "metadata": {},
   "source": [
    "The compilation time takes around 18min, but it only needs to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660603b-0734-4feb-9737-39873ccd7fb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c85ef-5604-4328-89ab-8954d1bcb98e",
   "metadata": {},
   "source": [
    "## Deploy compiled model to AWS Inferentia2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e67a89-7be2-451e-9ca9-8dab696ae93c",
   "metadata": {},
   "source": [
    "After compilation, we have to push the inference and requirements files to the same s3 bucket where the model files are.\n",
    "The final structure will be as follows:\n",
    "\n",
    "```bash\n",
    "s3://bucket/output/job-name/output/model/\n",
    "├── enc.neuron          # Compiled encoder model\n",
    "├── dec.neuron          # Compiled decoder model\n",
    "├── proj.neuron         # Compiled projection model\n",
    "├── model.pt            # Dummy PyTorch model (for container validation)\n",
    "└── code/\n",
    "    ├── inference.py    # Custom inference handler\n",
    "    └── requirements.txt # Python dependencies\n",
    "```\n",
    "**Note: The container runs a validation check on model file. To do it, it looks for files with .pt or .pth extension, since we have few of them, we renamed them to have a .neuron extension and create a dummy model.pt just to pass the validation step. Ultimately what is loaded and served, are our .neuron files, which are the compiled versions of our model.**\n",
    "\n",
    "Let's define the inference.py file below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267f764-ff9c-4226-abec-eea16476a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/inference.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import os\n",
    "import io\n",
    "import librosa\n",
    "import logging\n",
    "import torch\n",
    "import types\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline, WhisperConfig\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutput\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Global variables set during model loading\n",
    "processor = None\n",
    "output_attentions = False\n",
    "\n",
    "\n",
    "def enc_f(self, input_features, attention_mask, **kwargs):\n",
    "    if hasattr(self, 'forward_neuron'):\n",
    "        out = self.forward_neuron(input_features, attention_mask)\n",
    "    else:\n",
    "        out = self.forward_(input_features, attention_mask, return_dict=True)\n",
    "    return BaseModelOutput(**out)\n",
    "\n",
    "\n",
    "def dec_f(self, input_ids, attention_mask=None, encoder_hidden_states=None, **kwargs):\n",
    "    global processor, output_attentions\n",
    "    if attention_mask is not None and encoder_hidden_states is None:\n",
    "        encoder_hidden_states, attention_mask = attention_mask, encoder_hidden_states\n",
    "\n",
    "    inp = [input_ids, encoder_hidden_states]\n",
    "    if inp[0].shape[1] > self.max_length:\n",
    "        raise Exception(f\"Sequence length {inp[0].shape[1]} exceeds max {self.max_length}\")\n",
    "\n",
    "    pad_size = torch.as_tensor(self.max_length - inp[0].shape[1])\n",
    "    inp[0] = F.pad(inp[0], (0, pad_size), \"constant\", processor.tokenizer.pad_token_id)\n",
    "\n",
    "    if hasattr(self, 'forward_neuron'):\n",
    "        out = self.forward_neuron(*inp)\n",
    "    else:\n",
    "        out = self.forward_(input_ids=inp[0], encoder_hidden_states=inp[1], return_dict=True, use_cache=False, output_attentions=output_attentions)\n",
    "\n",
    "    out['last_hidden_state'] = out['last_hidden_state'][:, :input_ids.shape[1], :]\n",
    "    if out.get('attentions') is not None:\n",
    "        out['attentions'] = torch.stack([torch.mean(o[:, :, :input_ids.shape[1], :input_ids.shape[1]], axis=2, keepdim=True) for o in out['attentions']])\n",
    "    if out.get('cross_attentions') is not None:\n",
    "        out['cross_attentions'] = torch.stack([torch.mean(o[:, :, :input_ids.shape[1], :], axis=2, keepdim=True) for o in out['cross_attentions']])\n",
    "    return BaseModelOutputWithPastAndCrossAttentions(**out)\n",
    "\n",
    "def proj_out_f(self, inp):\n",
    "    global processor\n",
    "    if inp.shape[1] > self.max_length:\n",
    "        raise Exception(f\"Sequence length {inp.shape[1]} exceeds max {self.max_length}\")\n",
    "\n",
    "    pad_size = self.max_length - inp.shape[1]\n",
    "\n",
    "    if pad_size > 0:\n",
    "        x = F.pad(inp, (0, 0, 0, pad_size), \"constant\", processor.tokenizer.pad_token_id)\n",
    "    else:\n",
    "        x = inp\n",
    "\n",
    "    if hasattr(self, 'forward_neuron'):\n",
    "        out = self.forward_neuron(x)\n",
    "    else:\n",
    "        out = self.forward_(x)\n",
    "    return out[:, :inp.shape[1], :]\n",
    "\n",
    "\n",
    "def model_fn(model_dir, context=None):\n",
    "    global processor, output_attentions\n",
    "\n",
    "    model_id = os.environ.get(\"MODEL_ID\", \"openai/whisper-large-v3\")\n",
    "    max_dec_len = int(os.environ.get(\"MAX_DEC_LEN\", \"448\"))\n",
    "    output_attentions = os.environ.get(\"OUTPUT_ATTENTIONS\", \"false\").lower() == \"true\"\n",
    "\n",
    "    # Create minimal config\n",
    "    config = WhisperConfig.from_pretrained(model_id)\n",
    "\n",
    "    # Use meta device to avoid allocating memory for weights\n",
    "    with torch.device('meta'):\n",
    "        model = WhisperForConditionalGeneration(config)\n",
    "\n",
    "    # Move only the necessary components to CPU\n",
    "    model.to_empty(device='cpu')\n",
    "\n",
    "    # Load neuron models directly\n",
    "    model.model.encoder.forward_neuron = torch.jit.load(os.path.join(model_dir, \"enc.neuron\"))\n",
    "    model.model.decoder.forward_neuron = torch.jit.load(os.path.join(model_dir, \"dec.neuron\"))\n",
    "    model.proj_out.forward_neuron = torch.jit.load(os.path.join(model_dir, \"proj.neuron\"))\n",
    "\n",
    "    # Set up forwards\n",
    "    model.model.encoder.forward = types.MethodType(enc_f, model.model.encoder)\n",
    "    model.model.decoder.forward = types.MethodType(dec_f, model.model.decoder)\n",
    "    model.proj_out.forward = types.MethodType(proj_out_f, model.proj_out)\n",
    "\n",
    "    model.model.decoder.max_length = max_dec_len\n",
    "    model.proj_out.max_length = max_dec_len\n",
    "\n",
    "    # Load processor last\n",
    "    processor = WhisperProcessor.from_pretrained(model_id)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        chunk_length_s=30\n",
    "    )\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def input_fn(input_data, content_type, context=None):\n",
    "    logger.info(f\"Processing input with content_type: {content_type}\")\n",
    "    if content_type == 'audio/x-audio':\n",
    "        audio_array, sr = librosa.load(io.BytesIO(input_data), sr=16000)\n",
    "        logger.info(f\"Audio loaded - sample rate: {sr}, shape: {audio_array.shape}\")\n",
    "        return audio_array\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported mime type: {content_type}. Supported: audio/x-audio\")\n",
    "\n",
    "\n",
    "def predict_fn(audio_array, asr_pipeline, context=None):\n",
    "    logger.info(f\"Starting inference on audio with shape: {audio_array.shape}\")\n",
    "    output = asr_pipeline(audio_array)\n",
    "    logger.info(f\"Inference completed - transcription length: {len(output['text'])}\")\n",
    "    return {\"transcription\": output[\"text\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c066b-00e1-4fef-bd10-583ef5d750cd",
   "metadata": {},
   "source": [
    "Create the dummy `model.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1940b9-f43e-4fa2-b5af-2b6da52365b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tiny dummy TorchScript model\n",
    "dummy_model = torch.jit.script(torch.nn.Linear(1, 1))\n",
    "torch.jit.save(dummy_model, 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e2d956-faaf-43d4-834e-07040a75c874",
   "metadata": {},
   "source": [
    "Now we can upload the files. The S3 location can be extracted from the `estimator.model_data` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e06fd-c7f1-404f-815b-618015793d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Extract S3 URI from model_data dict\n",
    "model_s3_uri = estimator.model_data['S3DataSource']['S3Uri']\n",
    "\n",
    "# Upload inference code to model/code/\n",
    "subprocess.run(['aws', 's3', 'cp', 'src/', f'{model_s3_uri}code/', '--recursive'])\n",
    "subprocess.run(['aws', 's3', 'cp', 'model.pt', f'{model_s3_uri}model.pt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b3282-db13-4a83-a3c8-2bc8e7f13e8b",
   "metadata": {},
   "source": [
    "Now let's deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce41aa0-ca6d-4af8-8daf-35ba66cf1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model = Model(\n",
    "    model_data=estimator.model_data,\n",
    "    role=role,\n",
    "    sagemaker_session=session,\n",
    "    entry_point='inference.py',\n",
    "    source_dir='src',\n",
    "    env={\n",
    "        'MODEL_ID': model_id,\n",
    "        'MAX_DEC_LEN': str(max_dec_len),\n",
    "        'NEURON_RT_NUM_CORES': str(tp_degree),\n",
    "        'SAGEMAKER_MODEL_SERVER_WORKERS': '1'\n",
    "    },\n",
    "    image_uri=f\"763104351884.dkr.ecr.{inf_region}.amazonaws.com/pytorch-inference-neuronx:2.7.0-neuronx-py310-sdk2.25.0-ubuntu22.04\"\n",
    ")\n",
    "\n",
    "model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5aa55-17dc-4bb9-9978-439fd7758b42",
   "metadata": {},
   "source": [
    "Deployment can take roughly take 8-10 minutes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc25d7-419e-45a3-9aa2-4b92169276a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type=\"ml.inf2.xlarge\"\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    model_data_download_timeout=3600, # it takes some time to download all the artifacts and load the model\n",
    "    container_startup_health_check_timeout=1800\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4084ad22-5c96-4167-9cf8-f3bd3ef60c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29ac8223-a340-432e-ae0c-a8bf56a2bb60",
   "metadata": {},
   "source": [
    "Play the audio file we gonna transcibe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d03f5-c812-4c92-8ee0-874183656fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "\n",
    "# Load and play\n",
    "audio, sr = librosa.load(\"mlk.flac\")\n",
    "ipd.Audio(audio, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bab0fb-e578-49d6-8149-b3c6cfd62770",
   "metadata": {},
   "source": [
    "Configure serializers for input and output. Input is an audio file and output it's transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1730f-9b02-4bb0-a0e3-7945d135dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import DataSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\t\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=model.endpoint_name,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "predictor.serializer = DataSerializer(content_type='audio/x-audio')\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80a03a-e456-4653-b6cf-94ac6f5f78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mlk.flac\", \"rb\") as f:\n",
    "\tdata = f.read()\n",
    "\n",
    "output = predictor.predict(data)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47a74b-ebfc-49aa-ab11-108f33ee6ae0",
   "metadata": {},
   "source": [
    "Measure the average transcription time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c76f83-a9f7-4ca4-b965-28ed572e35a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "iters = 10\n",
    "\n",
    "start = time.time()\n",
    "for i in range(0,iters):\n",
    "    predictor.predict(data)\n",
    "end = time.time()\n",
    "\n",
    "transcription_time = (end-start)/iters\n",
    "print(f\"average transcription time is: {transcription_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd5c8a-0e8a-486c-9d34-1a9fcf77b546",
   "metadata": {},
   "source": [
    "## Cost performance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eaf351-9ab7-4818-a3c3-94fdc0079a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = librosa.get_duration(path=\"mlk.flac\")\n",
    "print(f\"Audio duration: {duration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7660fdf-fe04-4ce7-8f94-413f2ff26074",
   "metadata": {},
   "source": [
    "At the moment `ml.inf2.xlarge` is not supported by pricing api so we set the price per hour manualy according to [AWS Pricing Calculator](https://aws.amazon.com/sagemaker/ai/pricing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee56fa4-1bcc-4f71-aaa6-2817db83d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "price=0.99 # USD/hour in us-east-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3a0fc-dae5-4f16-9a3c-4bf69f3227ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_to_transcribe_1_sec = price / (3600.0/transcription_time*duration)\n",
    "print(f\"Cost to transcribe 1 second of audio using Whisper on {instance_type}: {price_to_transcribe_1_sec} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a2784-8479-44dc-8aa0-722c61ab3b0f",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8405825-273a-42cc-9019-d36b20cf0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
