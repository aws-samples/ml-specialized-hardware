{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6af3d66-699c-42c7-996f-5abe7de8d1cc",
   "metadata": {},
   "source": [
    "# Deploy Whisper model on AWS Inference 2 using Optimum Neuron\n",
    "The easiest way to deploy model on neuron devices is to use [Optimum Neuron](https://huggingface.co/docs/optimum-neuron/en/index) library. Whisper is one of the already supported models\n",
    "\n",
    "First install dependencies and download test file. You can skip this step if you executed [01_Whisper_gpu](01_Whisper_gpu.ipynb) notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c207e-3c22-44cc-a80d-abd16ff526e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U sagemaker librosa\n",
    "!wget --no-check-certificate https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db89cf7-8a98-47d2-8211-5b4f2ede68b8",
   "metadata": {},
   "source": [
    "Remember to restart kernel after installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da4d16-57a2-4c96-a67d-56709db7b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import librosa\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "inf_region = 'us-east-2'\n",
    "\n",
    "session = sagemaker.Session(boto_session=boto3.Session(region_name=inf_region))\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "print(role)\n",
    "\n",
    "inf_bucket = session.default_bucket()\n",
    "print(inf_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f11772-2220-4e71-bb9d-1bc47032a064",
   "metadata": {},
   "source": [
    "## Compile model to neuron\n",
    "Before we deploy the model we need to compile it so it can run on neuron devices. To do that we will use training job on Amazon Sagemaker that will run the compilation script and export compiled model to s3.\n",
    "\n",
    "Let's start with creating `src` directory when we put requirements.txt file for the compilation job and compilation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2f627-28ac-4a9c-8916-4e2c5a033125",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4c183-7c45-4365-a47e-c25a04b2eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/requirements.txt\n",
    "--extra-index-url https://pip.repos.neuron.amazonaws.com\n",
    "optimum-neuron[neuronx]==0.3.0\n",
    "librosa==0.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfab27f-79f2-418f-bb13-6dfd1010225c",
   "metadata": {},
   "source": [
    "The code executed inside __main__ will be used to compile the model. However, the same script will then be used to deploy a SageMaker endpoint later. For the model deployment, only the methods defined before main will be used by SageMaker, for instance: __model_fn__, __predict_fn__, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d906e-9af8-447b-8dfd-39ba84929b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/compile.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import os\n",
    "import io\n",
    "import librosa\n",
    "import shutil\n",
    "import logging\n",
    "import argparse\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoProcessor\n",
    "from optimum.neuron import NeuronWhisperForConditionalGeneration, pipeline\n",
    "\n",
    "# Defines a function model_fn that loads a tokenizer and a model from the specified directory.\n",
    "def model_fn(model_dir, context=None):\n",
    "    processor = AutoProcessor.from_pretrained(model_dir)\n",
    "    neuron_model = NeuronWhisperForConditionalGeneration.from_pretrained(model_dir)\n",
    "    neuron_model.config.forced_decoder_ids = None\n",
    "    neuron_model.config.suppress_tokens = []\n",
    "    neuron_model.generation_config.forced_decoder_ids = None\n",
    "    neuron_model.generation_config._from_model_config = True\n",
    "\n",
    "    return pipeline(\n",
    "        task=\"automatic-speech-recognition\",\n",
    "        model=neuron_model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        chunk_length_s=30,\n",
    "        # batch_size=16,  # batch size for inference\n",
    "    )\n",
    "\n",
    "# Defines an input_fn function to process incoming requests.\n",
    "def input_fn(input_data, content_type, context=None):\n",
    "    if content_type == 'audio/x-audio':\n",
    "        # Direct audio bytes\n",
    "        audio_array, sr = librosa.load(io.BytesIO(input_data), sr=16000)\n",
    "        return audio_array\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported mime type: {content_type}. Supported: audio/x-audio\")    \n",
    "\n",
    "# Defines a predict_fn function that generates predictions based on user input.\n",
    "def predict_fn(audio_array, asr_pipeline, context=None):\n",
    "    logging.info(\"starting inference\")\n",
    "    output = asr_pipeline(audio_array)\n",
    "    logging.info(f\"output: {output}\")\n",
    "    return {\"transcription\": output[\"text\"]}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"Number of samples processed in each batch during training or inference\")\n",
    "    parser.add_argument(\"--max_seq_len\", type=int, default=448, help=\"Maximum sequence length for input data\")\n",
    "    parser.add_argument(\"--hf_token\", type=str, default=None, help=\"Which is used for authentication with Hugging Face's model hub\")\n",
    "    parser.add_argument(\"--model_id\", type=str, default=\"meta-llama/Llama-3.2-1B\", help=\"Specifies the id for the pre-trained model to be used\")\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])    \n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    if not args.hf_token is None and len(args.hf_token) > 0:\n",
    "        print(\"HF token defined. Logging in...\")\n",
    "        login(token=args.hf_token)\n",
    "\n",
    "    compiler_args = {\"auto_cast\": \"all\", \"auto_cast_type\": \"bf16\"}\n",
    "    input_shapes = {\"batch_size\": args.batch_size, \"sequence_length\": args.max_seq_len}\n",
    "    model = NeuronWhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-large-v3\",\n",
    "        export=True,\n",
    "        inline_weights_to_neff=False,\n",
    "        **compiler_args,\n",
    "        **input_shapes,\n",
    "    )\n",
    "    # Save locally\n",
    "    model.save_pretrained(args.model_dir)\n",
    "\n",
    "    code_path = os.path.join(args.model_dir, 'code')\n",
    "    os.makedirs(code_path, exist_ok=True)\n",
    "\n",
    "    shutil.copy(__file__, os.path.join(code_path, \"inference.py\"))\n",
    "    shutil.copy('requirements.txt', os.path.join(code_path, 'requirements.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43151f43-6556-4dd5-928e-2f0c2f20d548",
   "metadata": {},
   "source": [
    "Define the training job and run it. We are using trn1.2xlarge instance because compilation requires extra amount of memory then running the model. We will use AWS Inference 2 later to deploy already compiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3dcf40-c0d7-4545-aa33-9b8b5ef770e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "HF_TOKEN=\"\"\n",
    "tp_degree=1\n",
    "batch_size=1\n",
    "# since compilation needs inputs size to be fixed we need to specify the max output for the decoder.\n",
    "max_seq_len=44\n",
    "\n",
    "# optimum-neuron 0.3.0 requires neuronxcc-2.19.8089 which is sdk2.24.1\n",
    "image_uri=f\"763104351884.dkr.ecr.{inf_region}.amazonaws.com/pytorch-training-neuronx:2.7.0-neuronx-py310-sdk2.24.1-ubuntu22.04\"\n",
    "\n",
    "hyperparameters={\n",
    "    \"max_seq_len\": max_seq_len,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"model_id\": \"openai/whisper-large-v3\"\n",
    "}\n",
    "\n",
    "if HF_TOKEN and len(HF_TOKEN) > 3:\n",
    "    hyperparameters[\"hf_token\"]= HF_TOKEN\n",
    "    \n",
    "estimator = PyTorch(\n",
    "    entry_point=\"compile.py\", # Specify your train script\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    sagemaker_session=session,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.2xlarge',\n",
    "    output_path=f\"s3://{inf_bucket}/output\",\n",
    "    disable_profiler=True,\n",
    "    disable_output_compression=True,\n",
    "\n",
    "\n",
    "    image_uri=image_uri,\n",
    "    env={\n",
    "        'NEURON_RT_NUM_CORES': str(tp_degree)\n",
    "    },\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001925f2-3d2f-432f-8828-1fe2f0907c0a",
   "metadata": {},
   "source": [
    "Compilation will take roughly 15-20 minutes.\n",
    "\n",
    "_NOTE: sometimes it fails in this instance size due to OOM errors, if that happens, you can try to compile in a instance with more RAM, i.e. inf2.8xlarge (verify thay your AWS Account Service Quota allows to use this type of instance._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660603b-0734-4feb-9737-39873ccd7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172c180-08cb-47d8-85c9-ce79715c1ded",
   "metadata": {},
   "source": [
    "## Deploy compiled model to AWS Inferentia2\n",
    "First let's get where the compiled model is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e06fd-c7f1-404f-815b-618015793d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data=estimator.model_data\n",
    "print(estimator.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a20c6ba-7e5c-4156-9e25-8b025ef806e6",
   "metadata": {},
   "source": [
    "If you have already a pre-compiled version of the model in S3, you can paste the S3_URI below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd22f1-46c1-4309-8518-929c3595fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "precompiled_s3_uri = 'PASTE_S3_URI_HERE'\n",
    "model_data={'S3DataSource': {'S3Uri': precompiled_s3_uri, 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n",
    "print(estimator.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2f7368-3b56-45a2-b8ad-f6ddc63d83f4",
   "metadata": {},
   "source": [
    "Now let deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d597c3a9-db97-4c8b-83eb-82e642fdbd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "\n",
    "print(f\"Model data: {model_data}\")\n",
    "\n",
    "instance_type=\"ml.inf2.xlarge\"\n",
    "num_workers=1\n",
    "\n",
    "image_uri=f\"763104351884.dkr.ecr.{inf_region}.amazonaws.com/pytorch-inference-neuronx:2.7.0-neuronx-py310-sdk2.24.1-ubuntu22.04\"\n",
    "\n",
    "print(f\"Instance type: {instance_type}. Num SM workers: {num_workers}\")\n",
    "pytorch_model = PyTorchModel(\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    name=name_from_base('whisper-neuronx'),\n",
    "    sagemaker_session=session,\n",
    "    container_log_level=logging.DEBUG,\n",
    "    model_server_workers=num_workers,\n",
    "    framework_version=\"2.1.2\",\n",
    "    env = {\n",
    "        'SAGEMAKER_MODEL_SERVER_TIMEOUT': '3600',\n",
    "        'MAX_SEQ_LEN': str(max_seq_len),\n",
    "        'NEURON_RT_NUM_CORES': str(tp_degree)\n",
    "    }\n",
    "    # for production it is important to define vpc_config and use a vpc_endpoint\n",
    "    #vpc_config={\n",
    "    #    'Subnets': ['<SUBNET1>', '<SUBNET2>'],\n",
    "    #    'SecurityGroupIds': ['<SECURITYGROUP1>', '<DEFAULTSECURITYGROUP>']\n",
    "    #}\n",
    ")\n",
    "pytorch_model._is_compiled_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4a6d1c-c456-4103-a83c-8876cde424bf",
   "metadata": {},
   "source": [
    "Deployment can take roughly take 8-10 minutes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1010d24-eef4-46f1-b70c-a4319671612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    model_data_download_timeout=3600, # it takes some time to download all the artifacts and load the model\n",
    "    container_startup_health_check_timeout=1800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ad497-bed2-42c6-b6b2-e8799ede5df6",
   "metadata": {},
   "source": [
    "Play the audio file we gonna transcibe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d03f5-c812-4c92-8ee0-874183656fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "\n",
    "# Load and play\n",
    "audio, sr = librosa.load(\"mlk.flac\")\n",
    "ipd.Audio(audio, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec0bab-684f-4e8d-a4d8-5ad42031d5dd",
   "metadata": {},
   "source": [
    "Configure serializers for input and output. Input is an audio file and output it's transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1730f-9b02-4bb0-a0e3-7945d135dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import DataSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\t\n",
    "predictor.serializer = DataSerializer(content_type='audio/x-audio')\n",
    "predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5187be-8dd7-4984-a9c0-a9197f6e5818",
   "metadata": {},
   "source": [
    "Execute transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0bac28-764e-418d-aa27-3a39f3882796",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mlk.flac\", \"rb\") as f:\n",
    "\tdata = f.read()\n",
    "\n",
    "output = predictor.predict(data)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bb3d7-1a53-4702-8ab0-811d0d237665",
   "metadata": {},
   "source": [
    "Calculate average transcription time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab1903-6a81-41ed-889c-a835b175a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "iters = 10\n",
    "\n",
    "start = time.time()\n",
    "for i in range(0,iters):\n",
    "    predictor.predict(data)\n",
    "end = time.time()\n",
    "\n",
    "transcription_time = (end-start)/iters\n",
    "transcription_time\n",
    "print(f\"Average transcription time: {transcription_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f669eae-49cd-4eb1-9542-e235a4938d93",
   "metadata": {},
   "source": [
    "## Cost performance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3f7f6-f3a1-4268-9c91-f34eedeadb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = librosa.get_duration(path=\"mlk.flac\")\n",
    "print(f\"Audio duration: {duration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c1721-a5a5-4394-ac10-531d77b5f2e2",
   "metadata": {},
   "source": [
    "At the moment `ml.inf2.xlarge` is not supported by pricing api so we set the price per hour manualy according to [AWS Pricing Calculator](https://aws.amazon.com/sagemaker/ai/pricing/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee56fa4-1bcc-4f71-aaa6-2817db83d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "price=0.99 # USD/hour in us-east-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3a0fc-dae5-4f16-9a3c-4bf69f3227ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_to_transcribe_1_sec = price / (3600.0/transcription_time*duration)\n",
    "price_per_minute = price_to_transcribe_1_sec * 60\n",
    "print(f\"Cost to transcribe 1 second of audio using Whisper on {instance_type}: ${price_to_transcribe_1_sec:.6f} USD\")\n",
    "print(f\"Cost per minute: ${price_per_minute:.4f} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d2996-057f-4a15-aedf-c56661e42256",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb769f-743f-4bc7-9586-1ab4c7393a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
