{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Custom Operators\n",
    "This notebook demonstrates how to insert a NKI kernel as a custom operators into a PyTorch.\n",
    "\n",
    "## Using NKI kernels\n",
    "To register a NKI kernel registration, you need to call a decorated NKI function.\n",
    "\n",
    "Let’s examine a guiding example below where we randomly initialize two inputs, add them together, and then multiply the result by the two input tensors element-wise. This effectively calculates: `a * b * (a + b)`.\n",
    "\n",
    "We define a common NKI kernel for addition. For more information on the kernel, see [SPMD Tensor Addition](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/tutorials/spmd_tensor_addition.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuronxcc.nki as nki\n",
    "import neuronxcc.nki.language as nl\n",
    "\n",
    "@nki.jit\n",
    "def nki_tensor_add_kernel_(a_input, b_input):\n",
    "  \"\"\"NKI kernel to compute element-wise addition of two input tensors\n",
    "  \n",
    "  This kernel assumes strict input/output sizes can be uniformly tiled to [128,512]\n",
    "\n",
    "  Args:\n",
    "      a_input: a first input tensor\n",
    "      b_input: a second input tensor\n",
    "\n",
    "  Returns:\n",
    "      c_output: an output tensor\n",
    "  \"\"\"\n",
    "\n",
    "  # Create output tensor shared between all SPMD instances as result tensor\n",
    "  c_output = nl.ndarray(a_input.shape, dtype=a_input.dtype, buffer=nl.shared_hbm)\n",
    "\n",
    "  # Calculate tile offsets based on current 'program'\n",
    "  offset_i_x = nl.program_id(0) * 128\n",
    "  offset_i_y = nl.program_id(1) * 512\n",
    "\n",
    "  # Generate tensor indices to index tensors a and b\n",
    "  ix_, iy_ = nl.mgrid[0:128, 0:512]\n",
    "  ix = offset_i_x + ix_\n",
    "  iy = offset_i_y + iy_\n",
    "\n",
    "  # Load input data from device memory (HBM) to on-chip memory (SBUF)\n",
    "  # We refer to an indexed portion of a tensor as an intermediate tensor\n",
    "  a_tile = nl.load(a_input[ix, iy])\n",
    "  b_tile = nl.load(b_input[ix, iy])\n",
    "\n",
    "  # compute a + b\n",
    "  c_tile = a_tile + b_tile\n",
    "\n",
    "  # store the addition results back to device memory (c_output)\n",
    "  nl.store(c_output[ix, iy], value=c_tile)\n",
    "\n",
    "  # Transfer the ownership of `c_output` to the caller\n",
    "  return c_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "We can perform `(a + b) * a * b` using native PyTorch code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_xla.core import xla_model as xm\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "a = torch.randn(256, 1024, dtype=torch.float32).to(device)\n",
    "b = torch.randn(256, 1024, dtype=torch.float32).to(device)\n",
    "c = a + b\n",
    "out = a * b * c\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s replace the tensor addition (`c = a + b`) with a NKI kernel. To do this we replace the `+` operator with a call to the NKI kernel caller (`nki_tensor_add`), and everything else works as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nki_tensor_add(a_input, b_input):\n",
    "  \"\"\"NKI kernel caller to compute element-wise addition of two input tensors\n",
    "\n",
    "  This kernel caller lifts tile-size restriction, by applying the kernel on tiles of the inputs/outputs\n",
    "\n",
    "  Args:\n",
    "      a_input: a first input tensor, of shape [N*128, M*512]\n",
    "      b_input: a second input tensor, of shape [N*128, M*512]\n",
    "\n",
    "  Returns:\n",
    "      a tensor of shape [N*128, M*512], the result of a_input + b_input\n",
    "  \"\"\"\n",
    "\n",
    "  # The SPMD launch grid denotes the number of kernel instances.\n",
    "  # In this case, we use a 2D grid where the size of each invocation is 128x512\n",
    "  grid_x = a_input.shape[0] // 128\n",
    "  grid_y = a_input.shape[1] // 512\n",
    "\n",
    "  return nki_tensor_add_kernel_[grid_x, grid_y](a_input, b_input)\n",
    "\n",
    "device = xm.xla_device()\n",
    "a = torch.randn(256, 1024, dtype=torch.float32).to(device)\n",
    "b = torch.randn(256, 1024, dtype=torch.float32).to(device)\n",
    "c = nki_tensor_add(a, b) # calling a NKI kernel, instead of the built-in torch op\n",
    "out = a * b * c\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what happens under the hood when we compile the above code, we can print HLO IR graph generated by XLA by setting the `NEURON_FRAMEWORK_DEBUG` environment variable. For example, you may add the following lines to your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NEURON_FRAMEWORK_DEBUG'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `.pbtxt` file is then written in your run directory that has the corresponding human-readable HLO IR.\n",
    "\n",
    "Let’s examine the XLA output of this example. In line #5 we can identify that the tensor addition is now mapped to an HLO `custom-call` instruction, with `AwsNeuronCustomNativeKernel` as `custom_call_target`. The output of that `custom-call` is then consumed by the next instruction in line #6 as usual.\n",
    "\n",
    "```python\n",
    "ENTRY %SyncTensorsGraph.22 (p0.2: f32[256,1024], p1.2: f32[256,1024]) -> (f32[256,1024]) {\n",
    " %p1.2 = f32[256,1024]{1,0} parameter(1), frontend_attributes={neff_input_name=\"input1\"}\n",
    " %p0.2 = f32[256,1024]{1,0} parameter(0), frontend_attributes={neff_input_name=\"input0\"}\n",
    " %multiply = f32[256,1024]{1,0} multiply(f32[256,1024]{1,0} %p1.2, f32[256,1024]{1,0} %p0.2)\n",
    " %custom-call.2 = f32[256,1024]{1,0} custom-call(f32[256,1024]{1,0} %p1.2, f32[256,1024]{1,0} %p0.2), custom_call_target=\"AwsNeuronCustomNativeKernel\", api_version=API_VERSION_UNSPECIFIED, backend_config=\"...\")\n",
    " %multiply.1 = f32[256,1024]{1,0} multiply(f32[256,1024]{1,0} %multiply, f32[256,1024]{1,0} %custom-call.2)\n",
    " ROOT %tuple = (f32[256,1024]{1,0}) tuple(f32[256,1024]{1,0} %multiply.1), frontend_attributes={neff_output_names=\"output0\"}\n",
    "}\n",
    "```\n",
    "\n",
    "The Neuron compiler replaces the above custom-call with the corresponding NKI kernel implementation while optimizing the rest of the compute graph as usual. At the end of the compilation process, a single compiled binary NEFF file is generated representing the entire graph including the NKI kernel. For more information about NEFF files, see [Neuron Compiler](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NKI in training graphs\n",
    "\n",
    "If you are using NKI to implement a new operator in a training graph, you might need to make the new operator interplay with the `autograd` engine in the framework. To do this, in PyTorch, you can subclass the framework’s base operator class and implement both the `forward()` and `backward()` methods. The `autograd` engine then uses the `backward()` method when performing auto-differentiation. See Extending [torch.autograd](https://pytorch.org/docs/stable/notes/extending.html) in the PyTorch Docs for instructions on doing this in PyTorch.\n",
    "\n",
    "Let’s reuse the `nki_tensor_add` kernel from before and demonstrate how to train a simple compute graph `(a+b)*a*b` in PyTorch.\n",
    "\n",
    "## PyTorch\n",
    "\n",
    "We define a `NkiAddFunc` class, which leverages the `nki_tensor_add` kernel in its `forward()` function. The gradients of both input tensors in `y = a + b` are ones, so the `backward()` function propagates the `dy` gradients from the previous backward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla.core.xla_model as xm\n",
    "device = xm.xla_device()\n",
    "\n",
    "class NkiAddFunc(torch.autograd.Function):\n",
    "  @staticmethod\n",
    "  def forward(ctx, a, b):\n",
    "    return nki_tensor_add(a, b)\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, dy, *args):\n",
    "    # gradients for a and b\n",
    "    return dy, dy\n",
    "\n",
    "# now, let's define the compute graph\n",
    "a = torch.randn(256, 1024, dtype=torch.float32).to(device).detach().requires_grad_()\n",
    "b = torch.randn(256, 1024, dtype=torch.float32).to(device).detach().requires_grad_()\n",
    "c = NkiAddFunc.apply(a, b)\n",
    "out = a * b * c\n",
    "\n",
    "# here we define a (dummy) loss-function, in prep for backward propagation\n",
    "loss = out.sum()\n",
    "\n",
    "# lastly, let's invoke the auto-grad engine\n",
    "loss.backward()\n",
    "\n",
    "xm.mark_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Release the NeuronCore for the next notebook\n",
    "\n",
    "Before moving to the next notebook we need to release the NeuronCore. If we don't do this the next notebook will not be able to use the resources - you can also stop the kernel via the GUI.\n",
    "\n",
    "> When running the command in the next cell the notebook will give an error - this is to be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
