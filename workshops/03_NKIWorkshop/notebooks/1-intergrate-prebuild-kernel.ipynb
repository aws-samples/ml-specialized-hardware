{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b098ef96",
   "metadata": {},
   "source": [
    "# 1-Intergrate Prebuild Kernel\n",
    "This notebook demonstrates how to integrate a prebuilt NKI kernel in our code and compare this to an existing implementation\n",
    "\n",
    "> Special thanks to Hanno Bever (bevhanno@amazon.de) for providing the custom attention code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc6380",
   "metadata": {},
   "source": [
    "## Available NKI Kernels\n",
    "The source code of the kernels in the neuronxcc.nki.kernels namespace is available at the Github Repository [nki-samples](https://github.com/aws-neuron/nki-samples). They are optimized kernels from the Neuron Team serving as samples. The repository also contains numeric tests, performance benchmarks, as well as scripts to use them in real models.\n",
    "\n",
    "You are welcome to customize them to fit your unique workloads, and contributing to the repository by opening a PR. Note that these kernels are already being deployed as part of the Neuron stack. \n",
    "\n",
    "In this notebook we're going to use the [nki.kernels.flash_fwd](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.kernels.flash_fwd.html) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuronxcc import nki\n",
    "from neuronxcc.nki.kernels import flash_fwd as FlashAttentionForward\n",
    "\n",
    "_flash_fwd_call = nki.jit()(FlashAttentionForward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c822e",
   "metadata": {},
   "source": [
    "To compare the NKI kernel with normal Flash Attention we're going to use the following `PyTorch` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class AttentionOrginal(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dropout=0.0):\n",
    "        super().__init__()\n",
    "        context_dim = query_dim if context_dim is None else context_dim\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=query_dim,\n",
    "            num_heads=heads,\n",
    "            kdim=context_dim,\n",
    "            vdim=context_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        context = x if context is None else context\n",
    "        out = self.mha(x, context, context, need_weights=False)\n",
    "        return out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1081b95a",
   "metadata": {},
   "source": [
    "Intergrating this with our NKI code would look the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ddff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import torch\n",
    "\n",
    "from torch.nn.functional import linear\n",
    "from neuronxcc.nki.kernels.attention import FlashConfig\n",
    "\n",
    "\n",
    "class AttentionNki(nn.Module):\n",
    "    def __init__(self, mha: nn.MultiheadAttention):\n",
    "        super().__init__()\n",
    "        self.mha = mha\n",
    "\n",
    "    def _in_projection_packed(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        w: torch.Tensor,\n",
    "        b: Optional[torch.Tensor] = None,\n",
    "    ) -> List[torch.Tensor]:\n",
    "        r\"\"\"Perform the in-projection step of the attention operation, using packed weights.\n",
    "\n",
    "        Output is a triple containing projection tensors for query, key and value.\n",
    "\n",
    "        Args:\n",
    "            q, k, v: query, key and value tensors to be projected. For self-attention,\n",
    "                these are typically the same tensor; for encoder-decoder attention,\n",
    "                k and v are typically the same tensor. (We take advantage of these\n",
    "                identities for performance if they are present.) Regardless, q, k and v\n",
    "                must share a common embedding dimension; otherwise their shapes may vary.\n",
    "            w: projection weights for q, k and v, packed into a single tensor. Weights\n",
    "                are packed along dimension 0, in q, k, v order.\n",
    "            b: optional projection biases for q, k and v, packed into a single tensor\n",
    "                in q, k, v order.\n",
    "\n",
    "        Shape:\n",
    "            Inputs:\n",
    "            - q: :math:`(..., E)` where E is the embedding dimension\n",
    "            - k: :math:`(..., E)` where E is the embedding dimension\n",
    "            - v: :math:`(..., E)` where E is the embedding dimension\n",
    "            - w: :math:`(E * 3, E)` where E is the embedding dimension\n",
    "            - b: :math:`E * 3` where E is the embedding dimension\n",
    "\n",
    "            Output:\n",
    "            - in output list :math:`[q', k', v']`, each output tensor will have the\n",
    "                same shape as the corresponding input tensor.\n",
    "        \"\"\"\n",
    "        E = q.size(-1)\n",
    "        if k is v:\n",
    "            if q is k:\n",
    "                # self-attention\n",
    "                proj = linear(q, w, b)\n",
    "                # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "                proj = (\n",
    "                    proj.unflatten(-1, (3, E))\n",
    "                    .unsqueeze(0)\n",
    "                    .transpose(0, -2)\n",
    "                    .squeeze(-2)\n",
    "                    .contiguous()\n",
    "                )\n",
    "                return proj[0], proj[1], proj[2]\n",
    "            else:\n",
    "                # encoder-decoder attention\n",
    "                w_q, w_kv = w.split([E, E * 2])\n",
    "                if b is None:\n",
    "                    b_q = b_kv = None\n",
    "                else:\n",
    "                    b_q, b_kv = b.split([E, E * 2])\n",
    "                q_proj = linear(q, w_q, b_q)\n",
    "                kv_proj = linear(k, w_kv, b_kv)\n",
    "                # reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\n",
    "                kv_proj = (\n",
    "                    kv_proj.unflatten(-1, (2, E))\n",
    "                    .unsqueeze(0)\n",
    "                    .transpose(0, -2)\n",
    "                    .squeeze(-2)\n",
    "                    .contiguous()\n",
    "                )\n",
    "                return (q_proj, kv_proj[0], kv_proj[1])\n",
    "        else:\n",
    "            w_q, w_k, w_v = w.chunk(3)\n",
    "            if b is None:\n",
    "                b_q = b_k = b_v = None\n",
    "            else:\n",
    "                b_q, b_k, b_v = b.chunk(3)\n",
    "            return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        context = x if context is None else context\n",
    "\n",
    "        query, key, value = x, context, context\n",
    "\n",
    "        if self.mha.batch_first:\n",
    "            # make sure that the transpose op does not affect the \"is\" property\n",
    "            if key is value:\n",
    "                if query is key:\n",
    "                    query = key = value = query.transpose(1, 0)\n",
    "                else:\n",
    "                    query, key = (x.transpose(1, 0) for x in (query, key))\n",
    "                    value = key\n",
    "            else:\n",
    "                query, key, value = (x.transpose(1, 0) for x in (query, key, value))\n",
    "\n",
    "        # set up shape vars\n",
    "        tgt_len, bsz, embed_dim = query.shape\n",
    "        src_len, _, _ = key.shape\n",
    "\n",
    "        head_dim = self.mha.head_dim\n",
    "        num_heads = self.mha.num_heads\n",
    "        assert head_dim * self.mha.num_heads == embed_dim\n",
    "        assert key.shape == value.shape\n",
    "\n",
    "        q, k, v = self._in_projection_packed(\n",
    "            query, key, value, self.mha.in_proj_weight, self.mha.in_proj_bias\n",
    "        )\n",
    "\n",
    "        # reshape q, k, v for multihead attention and make them batch first\n",
    "        #\n",
    "        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "        k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "        v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "        # update source sequence length after adjustments\n",
    "        src_len = k.size(1)\n",
    "\n",
    "        q = q.view(bsz, num_heads, tgt_len, head_dim)\n",
    "        k = k.view(bsz, num_heads, src_len, head_dim)\n",
    "        v = v.view(bsz, num_heads, src_len, head_dim)\n",
    "\n",
    "        # *************************************************************************************************\n",
    "        # NKI Kernel replacement START\n",
    "        # *************************************************************************************************\n",
    "\n",
    "        q = q.permute(0, 1, 3, 2)\n",
    "        k = k.permute(0, 1, 3, 2)\n",
    "        v = v.permute(0, 1, 3, 2)\n",
    "\n",
    "        config = FlashConfig(\n",
    "            **{\"seq_tile_size\": 2048, \"training\": False, \"should_transpose_v\": True}\n",
    "        )\n",
    "        attn_output = _flash_fwd_call[bsz, self.mha.num_heads](\n",
    "            q, k, v, seed=None, logit_bias=None, use_causal_mask=False, config=config,\n",
    "        )\n",
    "        \n",
    "        attn_output = attn_output.reshape(bsz, num_heads, tgt_len, head_dim)\n",
    "\n",
    "        # *************************************************************************************************\n",
    "        # NKI Kernel replacement END\n",
    "        # *************************************************************************************************\n",
    "\n",
    "        attn_output = (\n",
    "            attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
    "        )\n",
    "\n",
    "        attn_output = torch.nn.functional.linear(\n",
    "            attn_output, self.mha.out_proj.weight, self.mha.out_proj.bias\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
    "\n",
    "        if self.mha.batch_first:\n",
    "            return attn_output.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956a6df",
   "metadata": {},
   "source": [
    "## Creating the modules\n",
    "Both the `AttentionOrginal` module and the `AttentionNki` will use `query_dim=256`, `heads=4`, `context_dim=256` and `dropout=0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a16dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA: 256 4 256 256 0.0\n",
    "mha_module_org = AttentionOrginal(256, heads=4, context_dim=256, dropout=0.0)\n",
    "mha_module_nki = AttentionNki(mha_module_org.mha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963afb3f",
   "metadata": {},
   "source": [
    "## Compiling the NKI module\n",
    "\n",
    "Using `torch-neuronx`, there are two ways that a model can be executed for inference:\n",
    "\n",
    "- **XLA LazyTensor Inference**: A model is executed on Neuron by calling `to()` to move `Parameter` and `Tensor` data using the `xm.xla_device()`. Executing operations uses torch [Lazy Tensor](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/inference/trace-vs-xla-lazytensor.html#xla-lazytensor) to record, compile, and execute the graph.\n",
    "\n",
    "- **(Recommended) Traced Inference**: A model is traced prior to inference using the [trace()](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/inference/api-torch-neuronx-trace.html#torch_neuronx.trace) API. This trace is similar to `torch.jit.trace()` but instead creates a Neuron-specific [TorchScript](https://pytorch.org/docs/stable/jit.html) artifact. This artifact provides improved performance and portability compared to XLA [Lazy Tensor](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/inference/trace-vs-xla-lazytensor.html#xla-lazytensor) inference.\n",
    "\n",
    "To learn more about both methods see [Comparison of Traced Inference versus XLA Lazy Tensor Inference (torch-neuronx)](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/inference/trace-vs-xla-lazytensor.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614fa5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch_neuronx\n",
    "\n",
    "os.environ[\"XLA_DISABLE_FUNCTIONALIZATION\"] = \"\"\n",
    "os.environ[\"NEURON_TRANSFER_WITH_STATIC_RING_OPS\"] = \"\"\n",
    "\n",
    "os.environ[\"NEURON_RT_NUM_CORES\"] = \"1\"\n",
    "os.environ[\"NEURON_CC_FLAGS\"] = (\n",
    "    \"--log_level=INFO --cache_dir=./neuron_cache --model-type=generic -O1\"\n",
    ")\n",
    "os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
    "\n",
    "COMPILER_WORKDIR_ROOT = \"compiler\"\n",
    "\n",
    "x = torch.randn([1, 8192, 256])\n",
    "context = torch.randn([1, 8192, 256])\n",
    "\n",
    "example_inputs = x, context\n",
    "\n",
    "mha_compiled_neuron = torch_neuronx.trace(\n",
    "    mha_module_nki,\n",
    "    example_inputs,\n",
    "    compiler_workdir=os.path.join(COMPILER_WORKDIR_ROOT, \"MHA\"),\n",
    "    compiler_args=[\"--model-type\", \"transformer\", \"--auto-cast\", \"all\", \"--auto-cast-type\", \"bf16\", \"--optlevel\", \"1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805aeab4",
   "metadata": {},
   "source": [
    "## Looking at the results\n",
    "Note that while running both examples the resulting output tensor is almost identical, differences in output can be explained by the different datatypes used but are very insignificant if they occur at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43558c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = torch.float\n",
    "\n",
    "x = torch.randn([1, 8192, 256], dtype=DTYPE)\n",
    "context = torch.randn_like(x)\n",
    "\n",
    "org_result = mha_module_org(x, context=context)\n",
    "print(org_result.shape)\n",
    "print(org_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf68edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = torch.bfloat16\n",
    "\n",
    "x = x.bfloat16()\n",
    "context = context.bfloat16()\n",
    "\n",
    "neuron_result = mha_compiled_neuron(x, context)\n",
    "print(neuron_result.shape)\n",
    "print(neuron_result.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f62de6",
   "metadata": {},
   "source": [
    "## Release the NeuronCore for the next notebook\n",
    "\n",
    "Before moving to the next notebook we need to release the NeuronCore. If we don't do this the next notebook will not be able to use the resources - you can also stop the kernel via the GUI.\n",
    "\n",
    "> When running the command in the next cell the notebook will give an error - this is to be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b590a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf849ce-2f7b-460a-a9e7-1f9841462613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
