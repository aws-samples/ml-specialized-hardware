{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - setup\n",
    "In this guide, we will implement a simple “Hello World” style NKI kernel and run it on a NeuronDevice (Trainium/Inferentia2 or beyond device). We will showcase how to invoke a NKI kernel standalone through NKI baremetal mode and also through ML frameworks (PyTorch). Before diving into kernel implementation, let’s make sure you have the correct environment setup for running NKI kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "You need a [Trn1](https://aws.amazon.com/ec2/instance-types/trn1/) or [Inf2](https://aws.amazon.com/ec2/instance-types/inf2/) instance set up on AWS to run NKI kernels on a NeuronDevice. Once logged into the instance, follow steps below to ensure you have all the required packages installed in your Python environment.\n",
    "\n",
    "NKI is shipped as part of the Neuron compiler package. To make sure you have the latest compiler package, see [Setup Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/index.html) for an installation guide.\n",
    "\n",
    "You can verify that NKI is available in your compiler installation by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuronxcc.nki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This attempts to import the NKI package. It will error out if NKI is not included in your Neuron compiler version or if the Neuron compiler is not installed. The import might take about a minute the first time you run it. Whenever possible, we recommend using local instance NVMe volumes instead of EBS for executable code.\n",
    "\n",
    "If you intend to run NKI kernels without any ML framework for quick prototyping, you will also need NumPy installed.\n",
    "\n",
    "To call NKI kernels from PyTorch, you also need to have torch_neuronx installed. For an installation guide, see PyTorch Neuron Setup. You can verify that you have torch_neuronx installed by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_neuronx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing your first NKI kernel\n",
    "In current NKI release, all input and output tensors must be passed into the kernel as device memory (HBM) tensors on a NeuronDevice. The body of the kernel typically consists of three main phases:\n",
    "\n",
    "1. Load the inputs from device memory to on-chip memory (SBUF).\n",
    "2. Perform the desired computation.\n",
    "3. Store the outputs from on-chip memory to device memory.\n",
    "\n",
    "For more details on the above terms, see [NKI Programming Model](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/programming_model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuronxcc import nki\n",
    "import neuronxcc.nki.language as nl\n",
    "\n",
    "@nki.jit\n",
    "def nki_tensor_add_kernel(a_input, b_input):\n",
    "    \"\"\"\n",
    "    NKI kernel to compute element-wise addition of two input tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check all input/output tensor shapes are the same for element-wise operation\n",
    "    assert a_input.shape == b_input.shape\n",
    "\n",
    "    # Check size of the first dimension does not exceed on-chip memory tile size limit,\n",
    "    # so that we don't need to tile the input to keep this example simple\n",
    "    assert a_input.shape[0] <= nl.tile_size.pmax\n",
    "\n",
    "    # Load the inputs from device memory to on-chip memory\n",
    "    a_tile = nl.load(a_input)\n",
    "    b_tile = nl.load(b_input)\n",
    "\n",
    "    # Specify the computation (in our case: a + b)\n",
    "    c_tile = nl.add(a_tile, b_tile)\n",
    "\n",
    "    # Create a HBM tensor as the kernel output\n",
    "    c_output = nl.ndarray(a_input.shape, dtype=a_input.dtype, buffer=nl.shared_hbm)\n",
    "\n",
    "    # Store the result to c_output from on-chip memory to device memory\n",
    "    nl.store(c_output, value=c_tile)\n",
    "\n",
    "    # Return kernel output as function output\n",
    "    return c_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the kernel\n",
    "Next, we will cover unique ways to run the above NKI kernel on a NeuronDevice:\n",
    "\n",
    "1. NKI baremetal: run NKI kernel with no ML framework involvement\n",
    "2. PyTorch: run NKI kernel as a PyTorch operator\n",
    "3. JAX: run NKI kernel as a JAX operator (not used in this workshop)\n",
    "\n",
    "All three run modes can call the same kernel function decorated with the `nki.jit` decorator as discussed above:\n",
    "```\n",
    "@nki.jit\n",
    "def nki_tensor_add_kernel(a_input, b_input):\n",
    "```\n",
    "The `nki.jit` decorator automatically chooses the correct run mode by checking the incoming tensor type:\n",
    "\n",
    "1. NumPy arrays as input: run in NKI baremetal mode\n",
    "2. PyTorch tensors as input: run in PyTorch mode\n",
    "3. JAX tensors: run in JAX mode\n",
    "\n",
    "See [nki.jit](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.jit.html) API doc for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NKI baremetal\n",
    "\n",
    "Baremetal mode expects input tensors of the NKI kernel to be NumPy arrays. The kernel also converts its NKI output tensors to NumPy arrays. To invoke the kernel, we first initialize the two input tensors `a` and `b` as NumPy arrays. Finally, we call the NKI kernel just like any other Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.ones((4, 3), dtype=np.float16)\n",
    "b = np.ones((4, 3), dtype=np.float16)\n",
    "\n",
    "# Run NKI kernel on a NeuronDevice\n",
    "c = nki_tensor_add_kernel(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Alternatively, we can decorate the kernel with [nki.baremetal](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.baremetal.html) or pass the `mode` parameter to the `nki.jit` decorator, `@nki.jit(mode='baremetal')`, to bypass the dynamic mode detection. See [nki.baremetal](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.baremetal.html) API doc for more available input arguments for the baremetal mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "\n",
    "To run the above `nki_tensor_add_kernel` kernel using PyTorch, we initialize the input and output tensors as PyTorch `device` tensors instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_xla.core import xla_model as xm\n",
    "\n",
    "nki_tensor_add_kernel_pytorch = nki.jit(nki_tensor_add_kernel, mode='torchxla')\n",
    "\n",
    "device = xm.xla_device()\n",
    "\n",
    "a = torch.ones((4, 3), dtype=torch.float16).to(device=device)\n",
    "b = torch.ones((4, 3), dtype=torch.float16).to(device=device)\n",
    "\n",
    "c = nki_tensor_add_kernel_pytorch(a, b)\n",
    "\n",
    "print(c)  # an implicit XLA barrier/mark-step (triggers XLA compilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Alternatively, we can pass the `mode='torchxla'` parameter into the `nki.jit` decorator to bypass the dynamic mode detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Release the NeuronCore for the next notebook\n",
    "\n",
    "Before moving to the next notebook we need to release the NeuronCore. If we don't do this the next notebook will not be able to use the resources - you can also stop the kernel via the GUI.\n",
    "\n",
    "> When running the command in the next cell the notebook will give an error - this is to be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
