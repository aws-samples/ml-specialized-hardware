{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Neuron Profile \n",
    "In this tutorial, we use Neuron Profile to view the execution trace of a NKI kernel captured on a NeuronCore. In doing so, we learn about:\n",
    "\n",
    "- Installation and usage of Neuron Profile.\n",
    "\n",
    "- Inspecting a detailed execution timeline of compute engine instructions and DMA engine activities generated from your NKI kernel.\n",
    "\n",
    "As background, [Neuron Profile](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-profile-user-guide.html) is the tool you need to visualize where time is being spent during kernel execution on NeuronDevices, which is crucial for identifying performance bottlenecks and opportunities of your kernel. Neuron Profile produces runtime execution data for every instruction executed on each compute engine and also every data movement activity completed by DMA engines. Neuron Profile also reports key performance metrics such as compute engine and memory bandwidth utilization, which allows developers to quickly find out the achieved hardware efficiency of their kernel. Profiling typically has near zero overhead thanks to the dedicated on-chip profiling hardware in NeuronDevices.\n",
    "\n",
    "## Profile a NKI Kernel\n",
    "\n",
    "### Install Neuron Profile\n",
    "Make sure you have the latest version of the `aws-neuronx-tools`, which includes updated profiling support for NKI kernels. Neuron Profile is included within this package and is installed to `/opt/aws/neuron/bin`.\n",
    "\n",
    "The `aws-neuronx-tools` package comes pre-installed on [Neuron DLAMIs](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/dlami/index.html). For detailed installation instructions see [Neuron Profile User Guide: Installation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-profile-user-guide.html#installation).\n",
    "\n",
    "### Profile using `neuron-profile capture`\n",
    "\n",
    "To profile a NKI kernel the required steps are (1) enable `NEURON_FRAMEWORK_DEBUG` to tell the compiler to save the `NEFF` file, (2) execute the NKI kernel to generate the `NEFF`, and (3) run `neuron-profile capture` to generate a `NTFF` profile. Each step is described in more detail below.\n",
    "\n",
    "We will profile a NKI kernel which computes the element-wise exponential of an input tensor of any 2D shape. The rest of this tutorial will use a performance profile generated from this kernel as an example. Full code of `prof-kernel.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile prof-kernel.py\n",
    "\"\"\"\n",
    "Example kernel used to demmonstrate Neuron Profile.\n",
    "\"\"\"\n",
    "import torch\n",
    "from neuronxcc import nki\n",
    "import neuronxcc.nki.language as nl\n",
    "import math\n",
    "import os\n",
    "os.environ[\"NEURON_FRAMEWORK_DEBUG\"] = \"1\"\n",
    "os.environ[\"NEURON_CC_FLAGS\"]= \" --disable-dge \"\n",
    "\n",
    "@nki.jit\n",
    "def tensor_exp_kernel_(in_tensor):\n",
    "  \"\"\"NKI kernel to compute elementwise exponential of an input tensor\n",
    "\n",
    "  Args:\n",
    "      in_tensor: an input tensor of ANY 2D shape (up to SBUF size)\n",
    "  Returns:\n",
    "      out_tensor: an output tensor of ANY 2D shape (up to SBUF size)\n",
    "  \"\"\"\n",
    "  out_tensor = nl.ndarray(in_tensor.shape, dtype=in_tensor.dtype,\n",
    "                          buffer=nl.shared_hbm)\n",
    "\n",
    "  sz_p, sz_f = in_tensor.shape\n",
    "\n",
    "  i_f = nl.arange(sz_f)[None, :]\n",
    "\n",
    "  for p in nl.affine_range(math.ceil(sz_p / nl.tile_size.pmax)):\n",
    "    # Generate tensor indices for the input/output tensors\n",
    "    # pad index to pmax, for simplicity\n",
    "    i_p = p * nl.tile_size.pmax + nl.arange(nl.tile_size.pmax)[:, None]\n",
    "\n",
    "    # Load input data from external memory to on-chip memory\n",
    "    # only read up to sz_p\n",
    "    in_tile = nl.load(in_tensor[i_p, i_f], mask=(i_p<sz_p))\n",
    "\n",
    "    # perform the computation\n",
    "    out_tile = nl.exp(in_tile)\n",
    "\n",
    "    # store the results back to external memory\n",
    "    # only write up to sz_p\n",
    "    nl.store(out_tensor[i_p, i_f], value=out_tile, mask=(i_p<sz_p))\n",
    "\n",
    "    return out_tensor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  from torch_xla.core import xla_model as xm\n",
    "  device = xm.xla_device()\n",
    "\n",
    "  in_tensor = torch.rand((250, 512), dtype=torch.float32).to(device=device)\n",
    "\n",
    "  out_tensor = tensor_exp_kernel_(in_tensor)\n",
    "  print(f\"output_nki={out_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To profile this NKI kernel, follow these steps:\n",
    "\n",
    "1. Enable Neuron debug output by setting the `NEURON_FRAMEWORK_DEBUG` environment variable. This will trigger the Neuron compiler to save the Neuron Executable File Format (NEFF) artifact to the current directory after compilation of your NKI kernel. The NEFF contains all hardware instructions required to execute your NKI kernel on a NeuronDevice, as well as metadata and debug info needed for profiling. For example, add the following lines to your NKI kernel source file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NEURON_FRAMEWORK_DEBUG\"] = \"1\"\n",
    "os.environ[\"NEURON_CC_FLAGS\"]= \" --disable-dge \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "Use the flag `--disable-dge` to temporarily disable a new compiler feature which is interfering with DMA debugging information display in neuron-profile. This is highly recommended to improve NKI performance debugging experience until we release a software fix for this issue.\n",
    "</blockquote>\n",
    "\n",
    "2. Compile your NKI kernel to create a NEFF in your current directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 prof-kernel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "Find your NEFF named similarly to `MODULE_0_SyncTensorsGraph.13_12659246067793504316.neff`.\n",
    "</blockquote>\n",
    "\n",
    "3. Profile the NEFF. This profiling step executes the NEFF on the NeuronDevice and records a raw execution trace into an Neuron Trace File Format (NTFF) artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!neuron-profile capture -n <path_to_neff> -s profile.ntff --profile-nth-exec=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will save your NTFF profile to `profile_exec_2.ntff`.\n",
    "\n",
    "<blockquote>\n",
    "The `--profile-nth-exec=2` option will profile your NEFF twice on the NeuronDevice and output a NTFF profile for the second iteration. This is recommended to avoid one-time warmup delays which can be seen in the first iteration of execution.\n",
    "</blockquote>\n",
    "\n",
    "In [View Neuron Profile UI](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/neuron_profile_for_nki.html#nki-view-neuron-profile-ui), we will view the profile in a user-friendly format using the Neuron Profile UI.\n",
    "\n",
    "### Profile using nki.benchmark\n",
    "\n",
    "You may also use the [nki.benchmark](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.benchmark.html) API to generate a NEFF and NTFF programmatically. One caveat is [nki.benchmark](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.benchmark.html) runs your NEFF without an ML framework in [nki.baremetal](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.baremetal.html) mode, so the input tensors to the kernel must be NumPy arrays instead of framework tensors such as `torch.Tensor`.\n",
    "\n",
    "Below is an example NKI kernel decorated by [nki.benchmark](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.benchmark.html). Full code of `prof-kernel-benchmark.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile prof-kernel-benchmark.py\n",
    "\"\"\"\n",
    "Example kernel used to demmonstrate Neuron Profile with nki.benchmark.\n",
    "\"\"\"\n",
    "from neuronxcc import nki\n",
    "from neuronxcc.nki.typing import tensor\n",
    "import neuronxcc.nki.language as nl\n",
    "import math\n",
    "\n",
    "\n",
    "@nki.benchmark(save_neff_name='file.neff', save_trace_name='profile.ntff')\n",
    "def tensor_exp_kernel_(in_tensor):\n",
    "  \"\"\"NKI kernel to compute elementwise exponential of an input tensor\n",
    "  Args:\n",
    "      in_tensor: an input tensor of ANY 2D shape (up to SBUF size)\n",
    "  Returns:\n",
    "      out_tensor: an output tensor of ANY 2D shape (up to SBUF size)\n",
    "  \"\"\"\n",
    "  out_tensor = nl.ndarray(in_tensor.shape, dtype=in_tensor.dtype,\n",
    "                          buffer=nl.shared_hbm)\n",
    "\n",
    "  sz_p, sz_f = in_tensor.shape\n",
    "  i_f = nl.arange(sz_f)[None, :]\n",
    "  for p in nl.affine_range(math.ceil(sz_p / nl.tile_size.pmax)):\n",
    "    # Generate tensor indices for the input/output tensors\n",
    "    # pad index to pmax, for simplicity\n",
    "    i_p = p * nl.tile_size.pmax + nl.arange(nl.tile_size.pmax)[:, None]\n",
    "    # Load input data from external memory to on-chip memory\n",
    "    # only read up to sz_p\n",
    "    in_tile = nl.load(in_tensor[i_p, i_f], mask=(i_p<sz_p))\n",
    "    # perform the computation\n",
    "    out_tile = nl.exp(in_tile)\n",
    "    # store the results back to external memory\n",
    "    # only write up to sz_p\n",
    "    nl.store(out_tensor[i_p, i_f], value=out_tile, mask=(i_p<sz_p))\n",
    "\n",
    "  return out_tensor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tensor_exp_kernel_(tensor[[250, 512], nl.float32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use [nki.benchmark](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/api/generated/nki.benchmark.html) to create a NEFF file and NTFF profile in your current directory, execute the example NKI kernel with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 prof-kernel-benchmark.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Release the NeuronCore for the next notebook\n",
    "\n",
    "Before moving to the next notebook we need to release the NeuronCore. If we don't do this the next notebook will not be able to use the resources - you can also stop the kernel via the GUI.\n",
    "\n",
    "> When running the command in the next cell the notebook will give an error - this is to be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
